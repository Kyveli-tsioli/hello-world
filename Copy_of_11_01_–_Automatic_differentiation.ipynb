{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of 11.01 – Automatic differentiation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyveli-tsioli/hello-world/blob/main/Copy_of_11_01_%E2%80%93_Automatic_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXbVZCfh5Olh"
      },
      "source": [
        "# Installation cell\n",
        "%%shell\n",
        "if ! command -v julia 2>&1 > /dev/null\n",
        "then\n",
        "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uKBM4zJ5OlX"
      },
      "source": [
        "using Pkg\n",
        "\n",
        "Pkg.add(Pkg.PackageSpec(;name=\"Flux\", version=v\"0.9.0\"))\n",
        "\n",
        "pkg\"add Images\"\n",
        "pkg\"add Metalhead\"\n",
        "\n",
        "pkg\"precompile\"\n",
        "\n",
        "using Flux\n",
        "using Images\n",
        "using Metalhead\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#automatic differentiation\n",
        "#each layer really depends on that layer inputs and that layers outputs\n",
        "#error term backprogate gradually through the network\n",
        "#conclusion: how about i implement a library for this?\n",
        "#abstract over all the mathematical calculations and build a for loop\n",
        "#useful abstraction\n",
        "#gpu cpu neural networks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900dqb9wKoXJ"
      },
      "source": [
        "# Automatic differentiation #\n",
        "\n",
        "While it is obvious when you start working with a multi-layer perceptron that each layer’s output only depends on it’s inputs, you soon also realise from your maths and code that the gradient with respect to its weights only depend on its input and backpropagated error – making it possible to easily implement layer in isolation. So, why not build a library or framework? You are not alone in thinking this way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfQvF9kU6Lvw"
      },
      "source": [
        "What do frameworks/libraries achieve?\n",
        "\n",
        "* Increase productivity\n",
        "* Enable hardware acceleration and scaling (more easily)\n",
        "* Lowers barrier to entry\n",
        "* Encapsulate “best practices”\n",
        "* Code sharing (becoming a lungua franca)\n",
        "\n",
        "Some potential bad things:\n",
        "\n",
        "* Limiting expressivity\n",
        "* “Technical debt”\n",
        "* Computational overhead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKC95YKcr_BI"
      },
      "source": [
        "Most frameworks/libraries that you encounter really do two things:\n",
        "\n",
        "1. Automatic differentation\n",
        "2. GPU/TPU offloading\n",
        "\n",
        "We will primarily touch upon the former, although the latter is interesting in its own right, it belongs in a high-performance computing class (besides, as much as I like low-level coding, I am actually not that good at the inner workings of GPUs/TPUs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqYRDsUh97k"
      },
      "source": [
        "## History time ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDrvSkr2tHdq"
      },
      "source": [
        "\n",
        "\n",
        "### SN/Lush by LeCun and Bottou (1987) ###\n",
        "\n",
        "[Lush](http://lush.sourceforge.net/) oldest one that I am aware of, with a release as recent as 2011!\n",
        "\n",
        "* Object-oriented Lisp\n",
        "* Layer objects, that are chained together\n",
        "* Generates C code\n",
        "* Used for the original LeNet!\n",
        "\n",
        "LeNet code example:\n",
        "\n",
        "```lisp\n",
        "(de new-lenet5 (image-height \n",
        "                image-width\n",
        "                ki0 kj0 si0 sj0 ki1 kj1 si1 sj1\n",
        "                hid output-size net-param)\n",
        "  (let ((table0 (full-table 1 6))\n",
        "        (table1 (int-array 60 2))\n",
        "        (table2 (full-table 16 hid)))\n",
        "    (table1 ()()\n",
        "            '(0 0  1 0  2 0\n",
        "                1 1  2 1  3 1\n",
        "                2 2  3 2  4 2\n",
        "                3 3  4 3  5 3\n",
        "                4 4  5 4  0 4\n",
        "                5 5  0 5  1 5\n",
        "\n",
        "                0 6  1 6  2 6  3 6\n",
        "                1 7  2 7  3 7  4 7\n",
        "                2 8  3 8  4 8  5 8\n",
        "                3 9  4 9  5 9  0 9\n",
        "                4 10  5 10  0 10  1 10\n",
        "                5 11  0 11  1 11  2 11\n",
        "\n",
        "                0 12  1 12   3 12  4 12\n",
        "                1 13  2 13   4 13  5 13\n",
        "                2 14  3 14   5 14  0 14\n",
        "\n",
        "                0 15  1 15  2 15  3 15  4 15  5 15 ))\n",
        "    (new net-cscscf\n",
        "         image-height image-width\n",
        "         ki0 kj0 table0 si0 sj0\n",
        "         ki1 kj1 table1 si1 sj1\n",
        "         ;; WARNING: those two numbers must be changed \n",
        "         ;; when image-height/image-width change\n",
        "         (/ (- (/ (- image-height (1- ki0)) si0) (1- ki1)) si1)\n",
        "         (/ (- (/ (- image-width (1- kj0)) sj0) (1- kj1)) sj1)\n",
        "         (full-table 16 hid)\n",
        "         output-size\n",
        "         net-param)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXoxrFEMdfhJ"
      },
      "source": [
        "#LISP\n",
        "#LeNet: CNN\n",
        "lisp^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyey8TLNvCmU"
      },
      "source": [
        "### Theano by Breuleux et al. (2008) ###\n",
        "\n",
        "[Theano](http://www.deeplearning.net/software/theano/) was (abandoned since 2017) Developed at the [Mila group in Québec](https://mila.quebec/en/), it is the first one that I know of that takes an algebraic approach.\n",
        "\n",
        "* Python-based\n",
        "* Builds a computation graph out of mathematical objects\n",
        "* Generated C and CUDA code\n",
        "\n",
        "[LeNet code example](https://github.com/suriyadeepan/theano/blob/master/code/04CNN/lenet.py):\n",
        "\n",
        "```python\n",
        "x = T.matrix('x')\n",
        "y = T.ivector('y')\n",
        "\n",
        "# convert input x to form (batch_size,1,28,28)\n",
        "layer0_input = x.reshape((batch_size,1,28,28))\n",
        "\n",
        "# setup random stream\n",
        "rng = np.random.RandomState(123455)\n",
        "\n",
        "# build layer0\n",
        "layer0 = ConvPoolLayer(rng=rng,input=layer0_input,\n",
        "                      image_shape=(batch_size,1,28,28),\n",
        "                      filter_shape=(20,1,5,5))\n",
        "\n",
        "## Layer 1 setup ##\n",
        "layer1 = ConvPoolLayer(rng=rng,input=layer0.output,\n",
        "                      image_shape=(batch_size,20,12,12),\n",
        "                      filter_shape=(50,20,5,5))\n",
        "\n",
        "## Layer 2 : Hidden Layer setup ##\n",
        "# layer1 output shape : batch_sizex50x4x4\n",
        "# layer2_h input shape req : batch_size x (50*4*4)\n",
        "layer2_h_input = layer1.output.flatten(2)\n",
        "# n_in = 50x4x4 pixels; n_out = 500 hidden nodes\n",
        "layer2_h = HiddenLayer(rng=rng,input=layer2_h_input,n_in=50*4*4,n_out=500)\n",
        "\n",
        "# Layer 3 : Output layer : LogisticRegression\n",
        "layer3_o = LogisticRegression(input=layer2_h.output,n_in=500,n_out=10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Vv9hwY1cXu"
      },
      "source": [
        "### Torch by Collobert et al. (2008) ###\n",
        "\n",
        "[Lua](http://torch.ch/) wrapper around a very clean C library. It is notable that there is overlap in personell between Torch and Lush.\n",
        "\n",
        "* Layers and high-level operations defined in Lua\n",
        "* No code generation, simply calls C/CUDA through its C API\n",
        "* Enjoyed widespread use at Facebook, DeepMind, etc. up until very recently\n",
        "\n",
        "[LeNet code example](https://github.com/gwgundersen/lenet5/blob/master/lenet.lua):\n",
        "\n",
        "```lua\n",
        "function lenet.new(geometry)\n",
        "\n",
        "    local nChannels = geometry[1]  -- B&W=1, RGB=3.\n",
        "    local imgWidth = geometry[2]\n",
        "    local imgHeight = geometry[3]\n",
        "\n",
        "    local model = nn.Sequential()\n",
        "\n",
        "    -----------------------------------\n",
        "    -- 2 ConvNet layers\n",
        "    -- Arguments:\n",
        "    --      input depth, output depth, kernel width, kernel height\n",
        "    --      [step width], [step height], [padding width], [padding height]\n",
        "    model:add(nn.SpatialConvolution(nChannels, N_FEATURE_MAPS_1ST_L,\n",
        "        FILTER_SIZE, FILTER_SIZE, STEP_SIZE, STEP_SIZE, PADDING, PADDING))\n",
        "    model:add(nn.SpatialMaxPooling(POOL_SIZE, POOL_SIZE, STEP_SIZE, STEP_SIZE))\n",
        "    model:add(nn.ReLU())\n",
        "    model:add(nn.SpatialConvolution(N_FEATURE_MAPS_1ST_L, N_FEATURE_MAPS_2ND_L,\n",
        "        FILTER_SIZE, FILTER_SIZE, STEP_SIZE, STEP_SIZE, PADDING, PADDING))\n",
        "    model:add(nn.SpatialMaxPooling(POOL_SIZE, POOL_SIZE, STEP_SIZE, STEP_SIZE))\n",
        "    model:add(nn.ReLU())\n",
        "\n",
        "    -- Programmatically compute output size.\n",
        "    local dummy_input  = torch.Tensor(nChannels, imgWidth, imgHeight)\n",
        "    model:forward(dummy_input)\n",
        "    local output_w = model.modules[6].output[1]:size()[1]  -- Convert tensor to value.\n",
        "    local output_h = model.modules[6].output[2]:size()[1]\n",
        "    print('<lenet> output (w, h):', output_w, output_h)\n",
        "\n",
        "    -----------------------------------\n",
        "    -- 1 fully-connected layer\n",
        "    local newShape = N_FEATURE_MAPS_2ND_L * output_w * output_h\n",
        "    model:add(nn.Reshape(newShape))\n",
        "    model:add(nn.Linear(newShape, WIDTH_FC_L))\n",
        "    model:add(nn.ReLU())\n",
        "\n",
        "    -----------------------------------\n",
        "    -- Output layer\n",
        "    model:add(nn.Linear(WIDTH_FC_L, N_CLASSES))\n",
        "\n",
        "    return model\n",
        "end\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y40r7XIUe228"
      },
      "source": [
        "collobert: deep learning revolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiU48uBr3CGz"
      },
      "source": [
        "### Enter the hype… ###\n",
        "\n",
        "An the “Deep Learning revolution” took off, the number of frameworks/libraries exploded:\n",
        "\n",
        "* DeCaf/Caffe (2013)\n",
        "* ApolloCaffe (2015)\n",
        "* Dali (2015)\n",
        "* Knet.jl (2015)\n",
        "* cnn/DyNet (2015)\n",
        "* Chainer (2015)\n",
        "* Hype (2015)\n",
        "* And more…\n",
        "\n",
        "These are all pretty similar. Heck, I myself made [one back in 2013 with a focus on composition in language](https://github.com/ninjin/nerv).\n",
        "\n",
        "However, noteable was that [cnn/DyNet](http://dynet.io/) introduced the concept of “dynamic auto-batching” and [Chainer](https://chainer.org/) introduced the term “define-by-run” (also referred to as “eager” execution)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fngx_v56fSN"
      },
      "source": [
        "### TensorFlow by Google (2015) ###\n",
        "\n",
        "[TensorFlow](https://www.tensorflow.org/), Python bindings (mainly) for a low-level computation graph implemented in C++. \n",
        "\n",
        "* Allows for models to be developed in Python and then deployed on other devices.\n",
        "* No code generation out of the box, but can be achieved using XLA that operates on the computation graph.\n",
        "* Pushed hard inside of Google Brain and DeepMind – completely replaced Torch inside of DeepMind in about a year.\n",
        "* Pretty much an industry standard, even if reseachers largely have been migrating since 2017 or so.\n",
        "\n",
        "It had the nickname “TensorSlow” (`import tensorflow as tensorslow`) for the first year or so, but it picked up speed once it got more powerful primitives to express more dynamic inputs.\n",
        "\n",
        "[LeNet code example](https://github.com/tensorflow/models/blob/master/research/slim/nets/lenet.py):\n",
        "\n",
        "```python\n",
        "def lenet(images, num_classes=10, is_training=False,\n",
        "          dropout_keep_prob=0.5,\n",
        "          prediction_fn=slim.softmax,\n",
        "          scope='LeNet'):\n",
        "  end_points = {}\n",
        "\n",
        "  with tf.variable_scope(scope, 'LeNet', [images]):\n",
        "    net = end_points['conv1'] = slim.conv2d(images, 32, [5, 5], scope='conv1')\n",
        "    net = end_points['pool1'] = slim.max_pool2d(net, [2, 2], 2, scope='pool1')\n",
        "    net = end_points['conv2'] = slim.conv2d(net, 64, [5, 5], scope='conv2')\n",
        "    net = end_points['pool2'] = slim.max_pool2d(net, [2, 2], 2, scope='pool2')\n",
        "    net = slim.flatten(net)\n",
        "    end_points['Flatten'] = net\n",
        "\n",
        "    net = end_points['fc3'] = slim.fully_connected(net, 1024, scope='fc3')\n",
        "    if not num_classes:\n",
        "      return net, end_points\n",
        "    net = end_points['dropout3'] = slim.dropout(\n",
        "        net, dropout_keep_prob, is_training=is_training, scope='dropout3')\n",
        "    logits = end_points['Logits'] = slim.fully_connected(\n",
        "        net, num_classes, activation_fn=None, scope='fc4')\n",
        "\n",
        "  end_points['Predictions'] = prediction_fn(logits, scope='Predictions')\n",
        "\n",
        "  return logits, end_points\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wciQ9VLQ6hs3"
      },
      "source": [
        "### PyTorch by Paszke et al. (2016) ###\n",
        "\n",
        "[PyTorch](http://pytorch.org/) is pretty much identical to Torch, but introduces a Python API.\n",
        "\n",
        "* The Torch leadership was (still?) heavily against any non-Lua approach, so PyTorch was made by a single intern at Facebook AI Research.\n",
        "* Has become the defacto standard in research as of 2018/2019, why do you think this is the case?\n",
        "\n",
        "[LeNet code example](https://github.com/kuangliu/pytorch-cifar/blob/master/models/lenet.py):\n",
        "\n",
        "```python\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1   = nn.Linear(16*5*5, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iowJEf57ZT-"
      },
      "source": [
        "## High-level categorisation ##\n",
        "\n",
        "Where does automatic differentiation take place?\n",
        "\n",
        "* Abstract layers\n",
        "* Computational graph\n",
        "* Step-by-step on the level of the programming language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7fmt6xY6mEB"
      },
      "source": [
        "There is an ongoing “battle” as to what the “best” level to operate on is. Here are some key recent developments:\n",
        "\n",
        "* JAX (Google, 2018), traces NumPy-esque Python code, turns into into XLA (a compiler for TensorFlow graphs), and turns it into code.\n",
        "* PyTorch has made a similar tracer public earlier this year.\n",
        "* Zygote.jl, also traces programmes, but does so on top of the programming language itself rather than a higher level abstraction.\n",
        "* Swift for TensorFlow, akin to what Zygote.jl achieves, but does so “statically” by the virue of Swift being a statically compiled programming language.\n",
        "\n",
        "But, why should we care as researchers and practitioners?\n",
        "\n",
        "Remember?:\n",
        "\n",
        "* Limiting expressivity\n",
        "* “Technical debt”\n",
        "* Computational overhead\n",
        "\n",
        "The way I see it, this is a wonderful time to work with Deep Learning as the number of clever engineers that are trying to solve “our issues” is simply enormous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt_E_1eDiBJ-"
      },
      "source": [
        "## Flux.jl ##\n",
        "\n",
        "[Flux](https://github.com/FluxML/Flux.jl) is the “standard” library for automatic differentiation in Julia, but there are others such as [TensorFlow.jl](https://github.com/malmaud/TensorFlow.jl) and [Knet.jl](https://github.com/denizyuret/Knet.jl). In relation to what we have talked about so far, it is most closely related to PyTorch, but due to some Julia magic it can be hooked up with Zygote.jl to do language-level tracing (this is in beta though) and due to how Julia interacts with LLVM, it can generate GPU/TPU code – unlike PyTorch, as far as I know."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fwD8JeHikmC"
      },
      "source": [
        "## Logistic regression ##\n",
        "\n",
        "Let us consider a very simple model, how do we express it in Flux?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woywRPlwqJ-C",
        "outputId": "45326781-27d5-4444-9eb1-65639bff8c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "logistic(x) = 1/(1 + exp(-x))\n",
        "\n",
        "# Assuming our “toy” dataset.\n",
        "logreg = Dense(2, 1, logistic)\n",
        "\n",
        "logreg([0.17, 0.4711])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tracked 1-element Array{Float32,1}:\n",
              " 0.5332073f0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asorKychEzCb"
      },
      "source": [
        "Okay, that was cheating, let us “spell it out”:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhY_3EWhFgFU",
        "outputId": "c1c2f03c-4331-49d5-b25f-68c20e1f12fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "W    = param(randn(2)/100)\n",
        "b    = param(0)\n",
        "f(x) = logistic(W'x + b)\n",
        "\n",
        "f([0.17, 0.4711])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49982126328092474 (tracked)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEc72-kdHqFd",
        "outputId": "f7124073-a100-4d86-9e7b-9ee5dfc9b981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "using Flux.Tracker: gradient\n",
        "gradient(f, [0.17, 0.4711])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([-0.000168639, -0.000318548] (tracked),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEIgixwxirD3"
      },
      "source": [
        "Okay, so what is this `param` and “tracked” mumbo jumbo?\n",
        "\n",
        "From [Tracker.jl](https://github.com/FluxML/Tracker.jl):\n",
        "\n",
        "1. [array.jl](https://github.com/FluxML/Tracker.jl/blob/master/src/lib/array.jl)\n",
        "2. [real.jl](https://github.com/FluxML/Tracker.jl/blob/master/src/lib/real.jl)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPtK2ScOHCm8"
      },
      "source": [
        "struct TrackedArray{T,N,A<:AbstractArray{T,N}} <: AbstractArray{T,N}\n",
        "  tracker::Tracked{A}\n",
        "  data::A\n",
        "  grad::A\n",
        "  TrackedArray{T,N,A}(t::Tracked{A}, data::A) where {T,N,A} = new(t, data)\n",
        "  TrackedArray{T,N,A}(t::Tracked{A}, data::A, grad::A) where {T,N,A} = new(t, data, grad)\n",
        "end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCp-sS2lHMgO"
      },
      "source": [
        "mutable struct TrackedReal{T<:Real} <: Real\n",
        "  data::T\n",
        "  tracker::Tracked{T}\n",
        "end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrFiLxIXlZsq"
      },
      "source": [
        "### A minor case study, making `maximum` and `minimum` differentiable ###\n",
        "\n",
        "Back in 2018 I was playing around with Flux, but noticed that there was no support for [`maximum`](https://docs.julialang.org/en/v1.0/base/collections/#Base.maximum) and [`minimum`](https://docs.julialang.org/en/v1.0/base/collections/#Base.minimum) that I needed for an embedding-based baseline model. So, [I added support for it and filed a pull requesh](https://github.com/FluxML/Flux.jl/pull/249) and I think it makes for an interesting example of how these libraries work.\n",
        "\n",
        "So, what are these functions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enFD1l4vnZZ2",
        "outputId": "2a82b99e-b76d-4453-d90e-5d94a8b61c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "A = [1 2; 3 4]\n",
        "\n",
        "@show maximum(A)\n",
        "@show maximum(A, dims=1)\n",
        "@show maximum(A, dims=2)\n",
        "\n",
        "nothing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum(A) = 4\n",
            "maximum(A, dims=1) = [3 4]\n",
            "maximum(A, dims=2) = [2; 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZlrJlfMnRYF",
        "outputId": "4ce2acb8-c62f-4956-d2d8-a87dd9124441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "A = [1 2; 3 4]\n",
        "\n",
        "@show minimum(A)\n",
        "@show minimum(A, dims=1)\n",
        "@show minimum(A, dims=2)\n",
        "\n",
        "nothing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minimum(A) = 1\n",
            "minimum(A, dims=1) = [1 2]\n",
            "minimum(A, dims=2) = [1; 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLAM7w8niyC"
      },
      "source": [
        "Okay, fine, how would one add support to Flux then?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ3hPVcHlLWK"
      },
      "source": [
        "# Forward\n",
        "Base.maximum(xs::TrackedArray)         = track(maximum, xs)\n",
        "Base.maximum(xs::TrackedArray, region) = track(maximum, xs, region)\n",
        "Base.minimum(xs::TrackedArray)         = track(minimum, xs)\n",
        "Base.minimum(xs::TrackedArray, region) = track(minimum, xs, region)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyEKVEnflWV1"
      },
      "source": [
        "Now, what would the backwards pass look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdPmPqhlOZ6"
      },
      "source": [
        "function back(::typeof(maximum), Δ, xs::TrackedArray)\n",
        "    Δ′    = zeros(xs.data)\n",
        "    _, i = findmax(xs.data)\n",
        "    Δ′[i] = Δ\n",
        "    @back(xs, Δ′)\n",
        "end\n",
        "function back(::typeof(maximum), Δ, xs::TrackedArray, region)\n",
        "    Δ′     = zeros(xs.data)\n",
        "    _, is  = findmax(xs.data, region)\n",
        "    Δ′[is] = Δ\n",
        "    @back(xs, Δ′)\n",
        "end\n",
        "function back(::typeof(minimum), Δ, xs::TrackedArray)\n",
        "    Δ′    = zeros(xs.data)\n",
        "    _, i  = findmin(xs.data)\n",
        "    Δ′[i] = Δ\n",
        "    @back(xs, Δ′)\n",
        "end\n",
        "function back(::typeof(minimum), Δ, xs::TrackedArray, region)\n",
        "    Δ′     = zeros(xs.data)\n",
        "    _, is  = findmin(xs.data, region)\n",
        "    Δ′[is] = Δ\n",
        "    @back(xs, Δ′)\n",
        "end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMzRiq-Sn4Xa"
      },
      "source": [
        "There is a memory vs computation tradeoff made here, can you spot it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztLSGA93oDit"
      },
      "source": [
        "This “patch” has now been in Flux for a few releases and `maximum` and `minimum` works as expected!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltP70J2jIM_q",
        "outputId": "5bdb1410-2907-4041-e9bf-71957d5fb56d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "A = param([1 2; 3 4])\n",
        "\n",
        "@show maximum(A)\n",
        "@show maximum(A, dims=1)\n",
        "@show maximum(A, dims=2)\n",
        "\n",
        "nothing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum(A) = 4.0 (tracked)\n",
            "maximum(A, dims=1) = [3.0 4.0] (tracked)\n",
            "maximum(A, dims=2) = [2.0; 4.0] (tracked)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQSwAx5GIPGJ"
      },
      "source": [
        "So, what does a “real” model look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpYaggJIMGqn",
        "outputId": "6a18b7ee-d0a2-4bf4-fae3-a59668b3b322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "using Flux, Flux.Data.MNIST, Statistics\n",
        "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
        "using Base.Iterators: repeated\n",
        "\n",
        "# Classify MNIST digits with a simple multi-layer-perceptron\n",
        "\n",
        "imgs = MNIST.images()\n",
        "# Stack images into one large batch\n",
        "X = hcat(float.(reshape.(imgs, :))...)\n",
        "\n",
        "labels = MNIST.labels()\n",
        "# One-hot-encode the labels\n",
        "Y = onehotbatch(labels, 0:9)\n",
        "\n",
        "m = Chain(\n",
        "  Dense(28^2, 32, relu),\n",
        "  Dense(32, 10),\n",
        "  softmax)\n",
        "\n",
        "loss(x, y) = crossentropy(m(x), y)\n",
        "\n",
        "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))\n",
        "\n",
        "dataset = repeated((X, Y), 200)\n",
        "evalcb = () -> @show(loss(X, Y))\n",
        "opt = ADAM()\n",
        "\n",
        "Flux.train!(loss, params(m), dataset, opt, cb = throttle(evalcb, 10))\n",
        "\n",
        "accuracy(X, Y)\n",
        "\n",
        "# Test set accuracy\n",
        "tX = hcat(float.(reshape.(MNIST.images(:test), :))...)\n",
        "tY = onehotbatch(MNIST.labels(:test), 0:9)\n",
        "\n",
        "accuracy(tX, tY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Downloading MNIST dataset\n",
            "└ @ Flux.Data.MNIST /root/.julia/packages/Flux/dkJUV/src/data/mnist.jl:24\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   469  100   469    0     0    590      0 --:--:-- --:--:-- --:--:--   589\n",
            "100 9680k  100 9680k    0     0  2902k      0  0:00:03  0:00:03 --:--:-- 6038k\n",
            "┌ Info: Downloading MNIST dataset\n",
            "└ @ Flux.Data.MNIST /root/.julia/packages/Flux/dkJUV/src/data/mnist.jl:24\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   469  100   469    0     0    622      0 --:--:-- --:--:-- --:--:--   622\n",
            "100 28881  100 28881    0     0  16742      0  0:00:01  0:00:01 --:--:-- 50491\n",
            "┌ Info: Downloading MNIST dataset\n",
            "└ @ Flux.Data.MNIST /root/.julia/packages/Flux/dkJUV/src/data/mnist.jl:24\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   467  100   467    0     0    623      0 --:--:-- --:--:-- --:--:--   622\n",
            "100 1610k  100 1610k    0     0   609k      0  0:00:02  0:00:02 --:--:-- 1198k\n",
            "┌ Info: Downloading MNIST dataset\n",
            "└ @ Flux.Data.MNIST /root/.julia/packages/Flux/dkJUV/src/data/mnist.jl:24\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   467  100   467    0     0    628      0 --:--:-- --:--:-- --:--:--   627\n",
            "100  4542  100  4542    0     0   2391      0  0:00:01  0:00:01 --:--:-- 4435k\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss(X, Y) = 2.349295f0 (tracked)\n",
            "loss(X, Y) = 2.3335562f0 (tracked)\n",
            "loss(X, Y) = 2.3185475f0 (tracked)\n",
            "loss(X, Y) = 2.3042197f0 (tracked)\n",
            "loss(X, Y) = 2.2904954f0 (tracked)\n",
            "loss(X, Y) = 2.2772849f0 (tracked)\n",
            "loss(X, Y) = 2.2645047f0 (tracked)\n",
            "loss(X, Y) = 2.252074f0 (tracked)\n",
            "loss(X, Y) = 2.2399235f0 (tracked)\n",
            "loss(X, Y) = 2.2280014f0 (tracked)\n",
            "loss(X, Y) = 2.2162662f0 (tracked)\n",
            "loss(X, Y) = 2.2046838f0 (tracked)\n",
            "loss(X, Y) = 2.1932302f0 (tracked)\n",
            "loss(X, Y) = 2.1818867f0 (tracked)\n",
            "loss(X, Y) = 2.170641f0 (tracked)\n",
            "loss(X, Y) = 2.159475f0 (tracked)\n",
            "loss(X, Y) = 2.1483757f0 (tracked)\n",
            "loss(X, Y) = 2.1373317f0 (tracked)\n",
            "loss(X, Y) = 2.126334f0 (tracked)\n",
            "loss(X, Y) = 2.1153736f0 (tracked)\n",
            "loss(X, Y) = 2.1044407f0 (tracked)\n",
            "loss(X, Y) = 2.0935295f0 (tracked)\n",
            "loss(X, Y) = 2.0826368f0 (tracked)\n",
            "loss(X, Y) = 2.0717576f0 (tracked)\n",
            "loss(X, Y) = 2.060886f0 (tracked)\n",
            "loss(X, Y) = 2.050013f0 (tracked)\n",
            "loss(X, Y) = 2.0391347f0 (tracked)\n",
            "loss(X, Y) = 2.0282464f0 (tracked)\n",
            "loss(X, Y) = 2.0173416f0 (tracked)\n",
            "loss(X, Y) = 2.0064154f0 (tracked)\n",
            "loss(X, Y) = 1.9954637f0 (tracked)\n",
            "loss(X, Y) = 1.9844792f0 (tracked)\n",
            "loss(X, Y) = 1.9734604f0 (tracked)\n",
            "loss(X, Y) = 1.9624052f0 (tracked)\n",
            "loss(X, Y) = 1.9513077f0 (tracked)\n",
            "loss(X, Y) = 1.9401617f0 (tracked)\n",
            "loss(X, Y) = 1.9289598f0 (tracked)\n",
            "loss(X, Y) = 1.9176966f0 (tracked)\n",
            "loss(X, Y) = 1.9063716f0 (tracked)\n",
            "loss(X, Y) = 1.8949829f0 (tracked)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4576"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fix-wADHz8wP"
      },
      "source": [
        "## Extracurricular reading ##\n",
        "\n",
        "1. [“On Machine Learning and Programming Languages”](https://julialang.org/blog/2017/12/ml&pl) by Innes et al. (2017)\n",
        "2. [“Why Swift for TensorFlow”](https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md) by Lattner et al. (2018)\n",
        "3. [“A Differentiable Programming System to Bridge Machine Learning and Scientific Computing”](https://arxiv.org/abs/1907.07587) by Innes et al. (2019)\n",
        "3. [“Differentiable Programming Manifesto”](https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md) by Wei et al. (2019)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQn_5o0BriWh"
      },
      "source": [
        "## Acknowledgements ##\n",
        "\n",
        "A big thank you to James BRADBURY, a member of the Swift for TensorFlow team at Google AI – also, a Julia fanboy – for many useful discussions on these matters."
      ]
    }
  ]
}