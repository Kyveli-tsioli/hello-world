{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "13 – Convolutional neural networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyveli-tsioli/hello-world/blob/main/13_%E2%80%93_Convolutional_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHnPPzErnrr4"
      },
      "source": [
        "## Housekeeping ##\n",
        "\n",
        "### My absence ###\n",
        "\n",
        "Still not well, my apologies for this and thank you for your patience. An enormous amount of thanks to Pasquale for “stepping up” and helping.\n",
        "\n",
        "### Assignments ###\n",
        "\n",
        "1. Marking for assignment one is done!\n",
        "\n",
        "2. All assignments now released, please have a look\n",
        "\n",
        "3. To make group adjustments, make a post in the forum under “Week 6”\n",
        "\n",
        "4. Deadlines shifted now December 14th and January 18th.\n",
        "\n",
        "### Invited lectures ###\n",
        "\n",
        "Starting next week, still waiting on one speaker to finalise schedule.\n",
        "\n",
        "* Adversarial annotation\n",
        "* Graph neural networks\n",
        "* Variational autoencoders and other deep generative models\n",
        "* Deep reinforcement learning\n",
        "* Bayesian deep learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFFOwWNLLj4b"
      },
      "source": [
        "image real valued\n",
        "not rgb\n",
        "binary classification so far\n",
        "W1 takes our input transforms into some hidden layer representation\n",
        "bias for each and every single hidden representation\n",
        "then W2 we take this input space and transform into\n",
        "one bias for each output class. \n",
        "\n",
        "W1: association between a pixel and a weight\n",
        "input dimnension same as input image\n",
        "in binary case: the specific weight you want to assign\n",
        "with respet to a specific picture\n",
        "\n",
        "W2 about correlations in the detectors on the first layer\n",
        "\n",
        "sculture\n",
        "it is inside the image\n",
        "where this scultupre is occuring\n",
        "left handside/ middle etc\n",
        "we want the classifier to declare this\n",
        "mlpm cannot learn sth like transaltion innvariance\n",
        "you can learn it but you need to learn the weigths for each specific position\n",
        "the images are usually centerned\n",
        "\n",
        "rotation/viewpoint innvariance\n",
        "look the image from different angles\n",
        "\n",
        "size innvariance\n",
        "scale image up and down\n",
        "\n",
        "illumination innvariance\n",
        "\n",
        "key problems when dealing with image recognition\n",
        "\n",
        "techniques: \n",
        "translate your inputs: generate \n",
        "take your image and rotate, increase the training data that you have\n",
        "rotation of an image has an impact on classification of an image (see ecample with 7)\n",
        "\n",
        "pre deep learning\n",
        "shift features: mlp heuristic\n",
        "tries to find points of interest inside an image\n",
        "prunes the points in sets \n",
        "take image patches of equal siezes and use them for classifiers\n",
        "extract the patches and throw them to a linear classifier (logistic regression) or into an svm\n",
        "ignore the order of where the patches occured\n",
        "you get translation invariance by doing that \n",
        "what if we learn what is good from the image we have (data) :feature learning instead of coding it explicitly\n",
        "\n",
        "\n",
        "cnns:\n",
        "1998 the first cnn\n",
        "high level:\n",
        "some sort of input it generates through \n",
        "instead of one single W1 you have many different \n",
        "foot detector, eye detector, colour detector\n",
        "you apply the detector in many different places\n",
        "you learn this detectors (they are the weights)\n",
        "cnn:deep learning\n",
        "\n",
        "looks only at a specific region of the image \n",
        "then you move the convolutions into the areas\n",
        "feature map\n",
        "this correspond to each entry in this feature map=application of the detector\n",
        "is it a pool here?\n",
        "you get activations for each specific region inside the picture\n",
        "detectors=filters\n",
        "apply subsampling (max pooling)\n",
        "aggregates this info\n",
        "take a region, apply max, in this region what is the magnitute of the positive response in this region?\n",
        "apply these convolutions maps \n",
        "full connection=mlp\n",
        "\n",
        "5x5 pixel image\n",
        "what kind of filter can we apply?\n",
        "a 3x3 filter for example\n",
        "the size of the filter is a hyperparameter\n",
        "make this binary (the filter)\n",
        "how do we apply this?\n",
        "this is input image, this is the filter\n",
        "apply it to each region of the picture. (top left)\n",
        "0 goes to 1 etc\n",
        "move one step right and apply the filter\n",
        "feature map: tells us in the top left corner (1) there is a vertical line\n",
        "2 we have a shorter line\n",
        "a vertical line detector: each pixel in the filter\n",
        "different filters \\intstead of rotating\n",
        "poooling operation\n",
        "how you fit your weigths into this specific filter\n",
        "all lead to different feature maps\n",
        "\n",
        "we use a centric to construct the filter\n",
        "feature map -> input image\n",
        "\n",
        "padding: you add zeroes to be able to apply the filter all the way to the edge\n",
        "\n",
        "learn individual weights in terms of where these lines occur\n",
        "\n",
        "convulitional operation: apply detector to each and every point\n",
        "(is the cat here?)\n",
        "\n",
        "average pooling: 2x2 pooling\n",
        "average pooling: total sum of the activations in that specific region\n",
        "max pooling: max number for each coordinates\n",
        "\n",
        "what is the pooling operation in terms if logic:\n",
        "horizontal line detector\n",
        "if we do average pooling on the activations then what question we answering?\n",
        "on the average in this specific content of the input image how many horizontal lines we have?\n",
        "\n",
        "\n",
        "max pooling: what is the question we ask in the model\n",
        "in this specific coordinate what is the highest possible level of activation?\n",
        "if 3 for a specific coordinate: there is a horizontal line (ask of existence)\n",
        "\n",
        "\n",
        "convolution pooling convulotion pooling\n",
        "diagonal line detector\n",
        "of you train them is not noise\n",
        "the network learns to take these specific edges\n",
        "and apply them to eye etc\n",
        "\n",
        "\n",
        "transfer the style of the painting\n",
        "turn this image into this using a deep neural network\n",
        "the trick : they took 2 convulotional neural network\n",
        "they pretrained a neural networkforward pass \n",
        "take an input image, the starring item \n",
        "then calculate the specific activations\n",
        "all these feature maps\n",
        "and then take the difference of input layers\n",
        "took the gradient with respect to specific layers inside of the style image\n",
        "and then calculate the delta between the activations of thestar image and compare them with thes\n",
        "same activations in the input image\n",
        "adn then propagete them\n",
        "\n",
        "it gives only lifting dominant features\n",
        "\n",
        "what kind of invariance we see in the gif?\n",
        "translation invariance\n",
        "there are cnns that are rotation invariant\n",
        "data augmentation: take the input images and then rotate them 30 40 degrees and \n",
        "augment the magnitutde of the data this allows tyou to generalise better\n",
        "it helps\n",
        "cnns motivated by the problems that you observe\n",
        "the need for translation invariance for example\n",
        "\n",
        "the number of possible angles are many so \n",
        "the data can explode massively in data augmentation\n",
        "data augmentation: rotate the images and use them as new images\n",
        "feature maps think of them as activations\n",
        "\n",
        "if each weights corresponds to an rgb value\n",
        "you collapse them into rgb factors\n",
        "jpeg 3 dimensions\n",
        "gray scale images: hold for rgb imags as well\n",
        "(it is just an additional dimensions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900dqb9wKoXJ"
      },
      "source": [
        "# Convolutional neural networks #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mql7DZkD0cDS"
      },
      "source": [
        "### Reconsidering our image recognition approach so far ###\n",
        "\n",
        "Let us for a moment reconsider our 2-layer perceptron from a a few weeks ago:\n",
        "\n",
        "$$ \\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\} $$\n",
        "\n",
        "$$ x_i \\in \\mathbb{R}^{28 \\cdot 28} $$\n",
        "\n",
        "$$ y_i \\in \\{0, 1\\} $$\n",
        "\n",
        "$$ W_1 \\in \\mathbb{R}^{h \\times 28 \\cdot 28} $$\n",
        "\n",
        "$$ b_1 \\in \\mathbb{R}^{h} $$\n",
        "\n",
        "$$ W_2 \\in \\mathbb{R}^{1 \\times h} $$\n",
        "\n",
        "$$ b_2 \\in \\mathbb{R} $$\n",
        "\n",
        "$$ f(x) = \\sigma(W_2 * \\sigma(W_1 x + b_1) + b_2) $$\n",
        "\n",
        "$$ L(x, y) = P(y=\\hat{y}|x) $$\n",
        "\n",
        "How does each weight in $W_1$ relate to each input pixel? How does each weight in $W_2$ relate to each input pixel?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F4GiLdQ-XL6"
      },
      "source": [
        "\n",
        "## Invariance ###\n",
        "\n",
        "![](https://i.stack.imgur.com/iY5n5.png)\n",
        "\n",
        "[Image source](https://i.stack.imgur.com/iY5n5.png)\n",
        "\n",
        "As humans, we can clearly see that each of these images are of the same object. Thus, it is clearly desireable that the same would hold for a machine learning algorithm. Consider the multi-layer perceptron we just discussed, how would it react to say translating the image? Could you conceive of a way to make it robust to translation by modifying $\\mathcal{D}$?\n",
        "\n",
        "![](https://www.learnopencv.com/wp-content/uploads/2017/11/failure-mlp-mnist.jpg)\n",
        "\n",
        "[Image source](https://www.learnopencv.com/wp-content/uploads/2017/11/failure-mlp-mnist.jpg)\n",
        "\n",
        "Note here how the classification decision changes depending on the location of the digit inside the image, double-plus ungood indeed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5lXeuH7-Y2w"
      },
      "source": [
        "## Scale-Invariant Feature Transform (SIFT) ##\n",
        "\n",
        "Pre-deep learning approaches to address this.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/4/44/Sift_keypoints_filtering.jpg)\n",
        "\n",
        "[Image source](https://en.wikipedia.org/wiki/File:Sift_keypoints_filtering.jpg)\n",
        "\n",
        "Plenty of competitors published throughout the 00s:\n",
        "\n",
        "* Speeded Up Robust Features (SURF)\n",
        "* Gradient Location and Orientation Histogram\n",
        "* Etc.\n",
        "\n",
        "One would apply these feature extraction methods to an input image, then feed the resulting features into a linear classifier or SVM. Does this sound familiar? What is a disadvantage with this approach if we put our deep learning glasses on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Em6g80Bzu-o"
      },
      "source": [
        "## General architecture ##\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
        "\n",
        "[Image source](https://cdn-images-1.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
        "\n",
        "LeNet-5 by LeCun et al. (1998), the archetypical convolutional neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8EZSgtX67rf"
      },
      "source": [
        "### Filters or feature detectors ###\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-5.15.35-PM.png)\n",
        "\n",
        "[Image source](https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-5.15.35-PM.png)\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2017/08/Screen-Shot-2017-08-23-at-5.05.59-PM.png)\n",
        "\n",
        "[Image source](https://www.jeremyjordan.me/content/images/2017/08/Screen-Shot-2017-08-23-at-5.05.59-PM.png)\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-6.13.41-PM.png)\n",
        "\n",
        "[Image source](https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-6.13.41-PM.png)\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2017/07/no_padding_no_strides.gif)\n",
        "\n",
        "[Image source](https://github.com/vdumoulin/conv_arithmetic)\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2017/07/same_padding_no_strides.gif)\n",
        "\n",
        "[Image source](https://github.com/vdumoulin/conv_arithmetic)\n",
        "\n",
        "Note the padding to allow for the filter to operate near the edges of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJLSoAxc7ENd"
      },
      "source": [
        "## Pooling or “subsampling” ##\n",
        "\n",
        "![](http://www.embedded-vision.com/sites/default/files/technical-articles/CadenceCNN/Figure7.jpg)\n",
        "\n",
        "[Image source](http://www.embedded-vision.com/sites/default/files/technical-articles/CadenceCNN/Figure7.jpg)\n",
        "\n",
        "What does this achieve conceptually? What would happen if we only performed one round of convolution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23lie7xtZpoz"
      },
      "source": [
        "## Classification layer ##\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
        "\n",
        "[Image source](https://cdn-images-1.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
        "\n",
        "What about the last layer then? It is simply a good old multi-layer perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "betCSDXpZpyb"
      },
      "source": [
        "## A topological view ##\n",
        "\n",
        "![](https://1.bp.blogspot.com/-hoMkm-o_Lis/V0j1PegTSRI/AAAAAAAAEGE/knUuQfffTbggINk49msVYwM70D2qL4sKgCLcB/s1600/topological-visualisation-convolutional-neural-network.jpg)\n",
        "\n",
        "[Image source](https://1.bp.blogspot.com/-hoMkm-o_Lis/V0j1PegTSRI/AAAAAAAAEGE/knUuQfffTbggINk49msVYwM70D2qL4sKgCLcB/s1600/topological-visualisation-convolutional-neural-network.jpg)\n",
        "\n",
        "I found this recently and I think it illustrates very well how one can look at the filters operating on the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpabwukXZp-q"
      },
      "source": [
        "## “So what does it actually learn“ ##\n",
        "\n",
        "We are used to this question at this point, a whole lot it turns out. How would you relate this to SIFT and friends?\n",
        "\n",
        "![](http://i.stack.imgur.com/Hl2H6.png)\n",
        "\n",
        "[Image source](http://i.stack.imgur.com/Hl2H6.png)\n",
        "\n",
        "Composition of features, edges into pieces, and into items.\n",
        "\n",
        "![](https://elix-tech.github.io/images/2016/art/cnn.png)\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1200/1*HHUeLYdJosHkerJUNKWGrg.png)\n",
        "\n",
        "![](http://dataguild.org/wp-content/uploads/2017/04/science-and-art-via-cnn-09.png)\n",
        "\n",
        "I finally found a good illustration highlighting the inner workings of the method, note the gradient flows and how different layers are tied together. It is also worth noting that the convolutional network uses the same weights for all three images and that is is pre-trained for image classification."
      ]
    }
  ]
}