{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "Online Lecture 10.01: Recurrent neural networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyveli-tsioli/hello-world/blob/main/Online_Lecture_10_01_Recurrent_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXbVZCfh5Olh",
        "outputId": "e241630f-958b-4813-d0c0-b2a73ea1dbfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Installation cell\n",
        "%%shell\n",
        "if ! command -v julia 2>&1 > /dev/null\n",
        "then\n",
        "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 07:54:17--  https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz\n",
            "Resolving julialang-s3.julialang.org (julialang-s3.julialang.org)... 151.101.2.49, 151.101.66.49, 151.101.130.49, ...\n",
            "Connecting to julialang-s3.julialang.org (julialang-s3.julialang.org)|151.101.2.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 gce internal redirect trigger\n",
            "Location: https://storage.googleapis.com/julialang2/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz [following]\n",
            "--2020-11-05 07:54:17--  https://storage.googleapis.com/julialang2/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.122.128, 172.253.63.128, 142.250.31.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.122.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88706549 (85M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/julia.tar.gz’\n",
            "\n",
            "/tmp/julia.tar.gz   100%[===================>]  84.60M   115MB/s    in 0.7s    \n",
            "\n",
            "2020-11-05 07:54:18 (115 MB/s) - ‘/tmp/julia.tar.gz’ saved [88706549/88706549]\n",
            "\n",
            "   Cloning default registries into /root/.julia/registries\n",
            "   Cloning registry General from \"https://github.com/JuliaRegistries/General.git\"\n",
            "\u001b[2K\u001b[?25h Resolving package versions...\n",
            " Installed VersionParsing ── v1.2.0\n",
            " Installed Parsers ───────── v1.0.11\n",
            " Installed MbedTLS ───────── v0.6.8\n",
            " Installed BinaryProvider ── v0.5.10\n",
            " Installed Conda ─────────── v1.5.0\n",
            " Installed ZMQ ───────────── v1.1.0\n",
            " Installed IJulia ────────── v1.22.0\n",
            " Installed JSON ──────────── v0.21.1\n",
            " Installed SoftGlobalScope ─ v1.1.0\n",
            "  Updating `~/.julia/environments/v1.0/Project.toml`\n",
            "  [7073ff75] + IJulia v1.22.0\n",
            "  Updating `~/.julia/environments/v1.0/Manifest.toml`\n",
            "  [b99e7846] + BinaryProvider v0.5.10\n",
            "  [8f4d0f93] + Conda v1.5.0\n",
            "  [7073ff75] + IJulia v1.22.0\n",
            "  [682c06a0] + JSON v0.21.1\n",
            "  [739be429] + MbedTLS v0.6.8\n",
            "  [69de0a69] + Parsers v1.0.11\n",
            "  [b85f4697] + SoftGlobalScope v1.1.0\n",
            "  [81def892] + VersionParsing v1.2.0\n",
            "  [c2297ded] + ZMQ v1.1.0\n",
            "  [2a0f44e3] + Base64 \n",
            "  [ade2ca70] + Dates \n",
            "  [8ba89e20] + Distributed \n",
            "  [7b1f6079] + FileWatching \n",
            "  [b77e0a4c] + InteractiveUtils \n",
            "  [76f85450] + LibGit2 \n",
            "  [8f399da3] + Libdl \n",
            "  [37e2e46d] + LinearAlgebra \n",
            "  [56ddb016] + Logging \n",
            "  [d6f4376e] + Markdown \n",
            "  [a63ad114] + Mmap \n",
            "  [44cfe95a] + Pkg \n",
            "  [de0858da] + Printf \n",
            "  [3fa0cd96] + REPL \n",
            "  [9a3f8284] + Random \n",
            "  [ea8e919c] + SHA \n",
            "  [9e88b42a] + Serialization \n",
            "  [6462fe0b] + Sockets \n",
            "  [8dfed614] + Test \n",
            "  [cf7118a7] + UUIDs \n",
            "  [4ec0a83e] + Unicode \n",
            "  Building Conda ──→ `~/.julia/packages/Conda/x5ml4/deps/build.log`\n",
            "  Building ZMQ ────→ `~/.julia/packages/ZMQ/ItfqT/deps/build.log`\n",
            "  Building MbedTLS → `~/.julia/packages/MbedTLS/X4xar/deps/build.log`\n",
            "  Building IJulia ─→ `~/.julia/packages/IJulia/a1SNk/deps/build.log`\n",
            "Precompiling project...\n",
            "Precompiling IJulia\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uKBM4zJ5OlX"
      },
      "source": [
        "using Pkg\n",
        "\n",
        "pkg\"add Plots\"\n",
        "\n",
        "pkg\"precompile\"\n",
        "\n",
        "using Plots\n",
        "\n",
        "\n",
        "\n",
        "#this is our first advanced architecture\n",
        "#a sequence of non linearities and standard matrix transformations\n",
        "#mlp are still primitive architectures in neural networks in general\n",
        "#fixed sized inputs (so far), we mean each input has the exact same dimensionality (assignm1: 28x28 all the inputs)\n",
        "#this is not always the case (images can be of different sizes)\n",
        "#variable length inputs\n",
        "#recurrent neural network: some sort of differentiable state machine\n",
        "# tracks some sort of previous state (h)\n",
        "# doesnt come due to our current input, it comes from the previous input at time state h(t-1)\n",
        "#x(t) input at t\n",
        "#produces an output h(t)\n",
        "#x(t) goes to a processing box it produces an h(t): a reccurent connection\n",
        "#each hidden state lies on a d -space\n",
        "# weight matrix lies dx2d (maps tow of these vectors into 2d vectors)\n",
        "#concatenation between previous hidden state and current input that go to our weight matrix, we apply a non linearity (tanh) and we produce an output\n",
        "#dim of h is a hyperparameter\n",
        "#x's tend to be either embeddings or 1-hot vectors \n",
        "#time series and nlp, reinforcement learning: recurrent neural networks\n",
        "#keeps track of a world state: predicting an action, updating the world, keep track of an agent\n",
        "#biomedical research, sequences of symbols\n",
        "#h(t):once we have an input x(t+1) we go to x(t)\n",
        "#which time step i am currently in\n",
        "#see it as we take simply take multiple copies of the same value \n",
        "#h0 goes as an output and passed thought A\n",
        "#each time steps (rectangular with the 2 circles) will use the same W (weight tighing)\n",
        "#the previous h passed on the next layer (A)\n",
        "#the first layer not getting an h (h(-1)) commonly its a vector of dimension 0 with zeros\n",
        "#mlp: a single layer takes an input of some dimensionality and produces an output of some other dimensionality (there is no right or wrong here, it is a transformation from which we can learn the weigthts, computational unit)\n",
        "#tanh: activation function\n",
        "#outputs are the same dimension as the input\n",
        "#W is 2D \n",
        "# domain of the output: tanh bounded between -1 and 1 (not as logistic function)\n",
        "#largest gradient at the origin (0) as the logistic\n",
        "#when you go far from the origin you have no gradient at all\n",
        "#ht=tanh(W[h(t-1),x(t)]): what kind of behavior we loose if we put logistic\n",
        "#you cant produce negatives (a state being reverted)\n",
        "#in temrs of what can be expressed on a hidden state\n",
        "#you are always bounded \n",
        "#you can produce negative outputs\n",
        "#problem of exploding gradients: exploding outputs \n",
        "#the logistic function has output domain [0,1] no matter weights and x you have, you can never have a negative output\n",
        "#vanilla tanh1: loss in terms of numerical precision\n",
        "#you can scale the logistic outputs by 2x-1 would that give the same as tanh (transformation)\n",
        "#vanishing gradients: key problem with rnns\n",
        "#phenomenon where due to non linearities and initialisations you got up woth vanishing gradients\n",
        "#input x0: you want to predict a grammatially dependend word on x0\n",
        "#you take the gradient\n",
        "#error term we respect to our state h3\n",
        "#push the error term down (backpropagation)\n",
        "#non linearity used to produce h0, h1, h2, h3\n",
        "#when you want to push the gradient back through\n",
        "#the gradient becomes weaker and weaker (\"backpropagation through time\")\n",
        "#activation function has multiple effects\n",
        "#turing completeness: able to compute any computable function\n",
        "#you can untie the weights, you have more weights to learn if you do this\n",
        "#tanh: has a larger gradient and that helps (when comparing with logistic)\n",
        "#h is a bit like a markov process except with activation funtions between every step\n",
        "#you can use these models in a similar way to hidden markov models#no loss/gradient from intermediate h\n",
        "#vanishing gradients problem: \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900dqb9wKoXJ"
      },
      "source": [
        "# Recurrent neural networks #\n",
        "\n",
        "It is time for our first “advanced” architecture!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnBe9Hmrv9aA"
      },
      "source": [
        "## End of intermission: The problem of variable length inputs ##\n",
        "\n",
        "So far all of the inputs to our neural networks have been “fixed size”, in that each $x$ has had the same dimensionality. But this is not always the case:\n",
        "\n",
        "* Language: Not all sentences have the same length\n",
        "* Time series: Varying history length\n",
        "\n",
        "There is also the issue of images of different sizes, but let us ignore that for now…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTOYTpwY2kqw"
      },
      "source": [
        "## The “vanilla” Recurrent Neural Network ##\n",
        "\n",
        "Conceptually we can think of a Recurrent Neural Network (RNN) as a state machine that takes its previous output $h_{t - 1}$, an input at a given time $t$, $x_t$, and produces an output $h_t$. Early in the literature they were illustrated precisely like a state machine, see the illustration below.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png)\n",
        "\n",
        "[Image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "$$ x_i \\in \\mathbb{R}^{d} $$\n",
        "\n",
        "$$ h_i \\in \\mathbb{R}^{d} $$\n",
        "\n",
        "$$ W \\in \\mathbb{R}^{d \\times 2d} $$\n",
        "\n",
        "$$ h_t = \\textrm{tanh}(W[h_{t - 1}, x_t]) $$\n",
        "\n",
        "However, these while these illustrations are simple and compact, it is easier to “unroll” an RNN when discussing its behaviour. Exactly as we can see below.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "[Image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "This is a very powerful model, even proven to be Turing complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6sjbvLV7Z8H"
      },
      "source": [
        "### Activation function reminder ###\n",
        "\n",
        "Just to refresh our memory, here are the two activations functions we will use throughout today’s lecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2prFkmr3_t9"
      },
      "source": [
        "tanhprime(x)     = 1 .- tanh(x).^2\n",
        "plt = plot(tanh, xlimits=(-5, 5), label=\"tanh\", legend=:bottomright)\n",
        "plt = plot!(plt, tanhprime, label=\"tanh'\")\n",
        "logistic(x)      = 1/(1 .+ exp.(-x))\n",
        "logisticprime(x) = logistic(x).*(1 .- logistic(x))\n",
        "plt = plot!(plt, logistic, label=\"logistic\")\n",
        "plot!(plt, logisticprime, label=\"logistic'\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#vanilla rnn: x(t)combined using a weight matrix w, apply activation function which produces a new h state\n",
        "#we learn the weights: it depends on what function we use (logistic function/regression etc)\n",
        "#rtsm\n",
        "#cell state (c) or long term memory: the state that we pass \n",
        "#gates: a forget gate is a set of weights wf f for forgeting, we apply it to the prrevious h(t-1) conatenate it with the x(t -the input, you add the bias, each gate has a unique bias and weight)\n",
        "#the output will be bounded between 0 and 1 because we used the sigmoid\n",
        "#elemnt wise multiplication with the previous state (h(t-1))\n",
        "#forget it (0) or maintain it (1) or scale it if between 0 and 1\n",
        "#input scaling factor i(t): Wi our input weights, we take our previous output, concatenate with current input x(t) add a bias, linear transformation and then logistic function\n",
        "#cts candidate activations\n",
        "#the bias can be added in the weight matrix\n",
        "#sell state update: what it does?\n",
        "#Ct is a vector\n",
        "#element wise calculations all of these \n",
        "#forget gate: d dimensional vector f(t) same dimensionality wth C(t-1):scale each and every component\n",
        "#it is a scaling (0,1) it comes from the logistic\n",
        "#output gate: another weight matrix \n",
        "#o(t): different weight matrix now\n",
        "#gain scaled between 0 and 1\n",
        "#linealry scale based on the output gate (h(t)=o(t)*tanh(Ct))\n",
        "#ht is our output that we pass in the next state\n",
        "#the regularisation of information at each step help with vanishing gradients \n",
        "#vanilla rnn: for each time step you project your entire state and scale your entire state before pass into the next step\n",
        "#dont have any weight matrix that directly with cell state \n",
        "#vanishing gradient: you want to have a gradient with respect to the input and this gradient needs to be backpropagated\n",
        "#output gate: we produce the output ot \n",
        "#ht is conditional on c\n",
        "#wf controls the amount of information how much you are supposed to forget inide the cell \n",
        "#ft is vector to apply to the previous \n",
        "#ft can be 1 or 0 : either preserves what is in the current cell or scales it\n",
        "#shared for each time step the weight matrices\n",
        "#activation function: logistic function\n",
        "#the use of 0_t is just to calculate h_t (ot to scale each component of the output)\n",
        "#turing completeness proof for the rnn\n",
        "#how complex behavior ltsm can learn?\n",
        "#Kapathy: softmax predicted the next character (blue red text in the notebook)\n",
        "#off diagonal strong interactions, in the diagonal the info is retained\n",
        "#vp-np you preserve information \n",
        "#bi derictional rnn : 1 rnn from left to right and 1 from right to left and then concatenate the output states in order to produce predictions\n",
        "#you carry out backpropagation as if the unrolled weights are all separate, and then you just add up all of those gradients and update the weights with that sum of gradients\n",
        "#gradient exploding can happen it depends on the initialisation\n",
        "#if you want to capture some of the long term relationships in language, then vanishing gradients is a problem\n",
        "#we initialise to 1 because we dont want to forget at first\n",
        "#tensorflow initialises at zero (probably) bf=0(no contribution at all, no bias towards initial forgetting)\n",
        "#bf=1 you end up remembering most of the cell states\n",
        "#we cant initialise all of our weight matrices using small value\n",
        "#bf=1 rather than 0\n",
        "#all the functions are element wise \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkX6MCC3_UuX"
      },
      "source": [
        "### Exploding gradient ###\n",
        "\n",
        "We note the $\\textrm{tanh}$ activation function being used for our “vanilla” RNN. Why is it necessary? Given the term “exploding gradient” what do you believe it signifies? When it is a problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhMfgPki4MrJ"
      },
      "source": [
        "### Vanishing gradient ###\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png)\n",
        "\n",
        "Imagine that an output $h_3$ is to be influenced by observations $x_0$ and $x_1$, this is exactly the kind of behaviour we expect an RNN to be able to learn.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)\n",
        "\n",
        "However, by virtue of the inner workings of the “vanilla” RNN. As the distance between an output and its related inputs increase, it becomes more difficult to learn the desired behaviour, sometimes even impossible. *Why?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUiejVx_5wsC"
      },
      "source": [
        "### Long Short-Term Memory ###\n",
        "\n",
        "The problem of vanishing gradients plagued the field througouht the 90s, but unlike the vanishing gradient for multi-layer perceptrons which we only came to terms with in the late 00s, [Hochreiter and Schmidhuber (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf) proposed a fairly ingenious solution to the problem – Long Short-Term Memory, commonly referred to as an LSTM.\n",
        "\n",
        "Consider our “vanilla” RNN.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
        "\n",
        "$$ h_t = \\textrm{tanh}(W[h_{t - 1}, x_t]) $$\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "### The “cell state”, our long-term memory ###\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)\n",
        "\n",
        "### Forget gate ###\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
        "\n",
        "### Input gate ###\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
        "\n",
        "### Cell state update ###\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
        "\n",
        "### Output gate ###\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
        "\n",
        "What are some positive downsides of the LSTM? Does it still suffer from exploding/vanishing gradients or is it immue to these phenomena under all circumstances? In 2016 (or was it 2015?) Google Research published a paper on a minute detail regarding the initialisation of the LSTM weights at ICML. Allegedly Schmidthuber upon seeing the work as a poster presentation commented to the authors that “Oh, yes, we forgot to mention that, didn’t we?”. What do you think this detail was?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-PlqvwSCqil"
      },
      "source": [
        "### LSTM variants ###\n",
        "\n",
        "There is no standard formulation of the LSTM, although what we used above is the most dominant. Here are two alternative formulations. Can we derive their motivations from their equations?\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png)\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png)\n",
        "\n",
        "In practice the “simpler” formulation is “powerful enough” and you rarely see these variants.\n",
        "\n",
        "#### Gated Recurrent Unit (GRU) ####\n",
        "\n",
        "Introduced by Cho et al. (2014) as a more computationally efficient variant to the LSTM. Again, once you are familiar with the LSTM it becomes fairly straight forward to analyse its behaviour from its equations.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnYoWJI-xNwL"
      },
      "source": [
        "### So, what does an LSTM learn? ###\n",
        "\n",
        "In theory, even a “vanilla” RNN is Turing complete ([Siegelmann and Sontag, 1995](http://research.cs.queensu.ca/home/akl/cisc879/papers/SELECTED_PAPERS_FROM_VARIOUS_SOURCES/05070215382317071.pdf)). But at this point, we can draw parallels to how we know that while neural networks are universal function approximators, we do know that ultimately we are limited by what we can learn given finite amounts of data and compute. So, how complex of a behaviour can these models learn?\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/40be3888daa5c2e5af4d36ae22f690bcc8caf600/5-Figure2-1.png)\n",
        "\n",
        "[Image source](https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Recurrent-Networks-Karpathy-Johnson/40be3888daa5c2e5af4d36ae22f690bcc8caf600)\n",
        "\n",
        "From [“Visualizing and Understanding Recurrent Networks”](https://arxiv.org/abs/1506.02078) by Karpathy et al. (2015), which is a remarkably accessible paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxXbTi6KxrW-"
      },
      "source": [
        "## Structure-guided recurrent neural networks ##\n",
        "\n",
        "For a brief moment in time, referred to as “recursive neural networks”.\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/0c23ebb3abf584fa5e0fde558584befc94fb5ea2/1-Figure1-1.png)\n",
        "\n",
        "[Image source](https://www.semanticscholar.org/paper/Parsing-Natural-Scenes-and-Natural-Language-with-Socher-Lin/9c0ddf74f87d154db88d79c640578c1610451eec)\n",
        "\n",
        "### Sentiment analysis ##\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/961f6d5bb9b43833d7387e377b25d7a68118b23b/3-Figure2-1.png)\n",
        "\n",
        "[Image source](https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600)\n",
        "\n",
        "More examples at the [Stanford Sentiment Treebank homepage](https://nlp.stanford.edu/sentiment/treebank.html).\n",
        "\n",
        "### Syntactic analysis  ###\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/5/54/Parse_tree_1.jpg)\n",
        "\n",
        "[Image source](https://en.wikipedia.org/wiki/File:Parse_tree_1.jpg)\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/acc4e56c44771ebf69302a06af51498aeb0a6ac8/1-Figure1-1.png)\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/acc4e56c44771ebf69302a06af51498aeb0a6ac8/9-Figure5-1.png)\n",
        "\n",
        "[Image source](https://www.semanticscholar.org/paper/Parsing-with-Compositional-Vector-Grammars-Socher-Bauer/acc4e56c44771ebf69302a06af51498aeb0a6ac8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxj4qG_XJNh4"
      },
      "source": [
        "### 2014 to 2018: RNN variants ###\n",
        "\n",
        "For a few years, RNNs with LSTM cells dominated the field of natural language processing. Below are two very common examples.\n",
        "\n",
        "#### Sequence to Sequence RNN ####\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/cd52da21cdec50b25b6fb0ba6741091ad38fc986/2-Figure1-1.png)\n",
        "\n",
        "#### Bi-directional RNN ####\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4/4-Figure3-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAMCZfJaTkGd"
      },
      "source": [
        "## Additional reading ##\n",
        "\n",
        "1. [“The Unreasonable Effectiveness of Recurrent Neural Networks”](https://karpathy.github.io/2015/05/21/rnn-effectiveness)\n",
        "2. [“The Unreasonable Syntactic Expressivity of RNNs”](https://nlp.stanford.edu/~johnhew/rnns-hierarchy.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhpkUU43UC1"
      },
      "source": [
        "## Acknowledgements ##\n",
        "\n",
        "A big thank you to Christopher Olah for his [“Understanding LSTM Networks”](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (2015) from which I myself and many others “steal” the excellent illustrations."
      ]
    }
  ]
}