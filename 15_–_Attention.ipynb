{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "15 – Attention",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyveli-tsioli/hello-world/blob/main/15_%E2%80%93_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900dqb9wKoXJ"
      },
      "source": [
        "# Attention #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRxcCwyRHDBB"
      },
      "source": [
        "## Revisiting the recurrent neural network ##\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "A useful perspective on recurrent neural networks is as we have discussed before, to see it as a very deep feed-forward neural network – think multi-layer perceptron – where the same weights are used at each layer.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)\n",
        "\n",
        "In terms of a learning problem, it becomes gradually more difficult (although **theoretically** not impossible) to learn relationships as the distance between the related input and output grows.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "\n",
        "The LSTM with its cell state does resolve the issue of vanishing gradients, but as we are about to see, challenges still remain when it comes to long-term dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io7rUYSmiGq_"
      },
      "source": [
        "LSTM : no vanishing gradients\n",
        "to relieve the problem of \n",
        "gradually more difficult to learn long distance dependencies "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EViC-ZNPh-WN"
      },
      "source": [
        "long distance dependencies: difficult to learn\n",
        "[link text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbkXm4raLqJ4"
      },
      "source": [
        "## Sequence-to-Sequence recurrent neural network (seq2seq) ##\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/cd52da21cdec50b25b6fb0ba6741091ad38fc986/2-Figure1-1.png)\n",
        "\n",
        "(From Sutskever et al. (2014))\n",
        "\n",
        "![](https://i.stack.imgur.com/Ux0Xh.png)\n",
        "\n",
        "(From Cho et al. (2014))\n",
        "\n",
        "Very briefly at the end of the recurrent neural network lecture I presented this neural network structure commonly referred to as “seq2seq”. It was actually discovered independently by two different groups back in 2014. Sutskever et al. (2014) at Google Brain and Cho et al. (2014) at Mila in Montréal – the latter being the original GRU paper.\n",
        "\n",
        "Here is a concrete realisation of this architecture for the task it was originally designed for: machine translation.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*iXV7BD2iKMBIG-tmyzx88g.jpeg)\n",
        "\n",
        "Formall, we have two functions. An *encoder* that takes a sequence of discrete symbols (or discrete symbols encoded as vectors, remember word2vec) and turns into into a low-dimensional representation of its information content:\n",
        "\n",
        "$$ f_{enc}(X) = \\textbf{h} $$\n",
        "\n",
        "Which it then passes to a decoder function that based on $\\textbf{h}$ decodes an output sequence $Y^{\\prime}$:\n",
        "\n",
        "$$ f_{dec}(\\textbf{h}) = Y^{\\prime} $$\n",
        "\n",
        "In a way, if $X = Y$, you can think of these as autoencoders with discrete, variable-length inputs and outputs. Not entirely surprisingly, they have been pre-trained in exactly this way (Dai and Le, 2015).\n",
        "\n",
        "Considering the model from a perspective we are already familiar with, we can see the encoder as nothing more than what we have already spoken about in terms of using a recurrent neural network to encode a variable-width sequence as a fixed-size output. The decoder component is performing language modelling akin to what we have already studied, the only real difference is that language modelling *conditioned* on a hidden state as well as what has already been generated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTz6u5SKiRST"
      },
      "source": [
        "2014 hits nlp\n",
        "word embeddings introduced\n",
        "1 problem nlp: frequently prob when you hae a sequence of words A B C and on the other side we have outputs\n",
        "ABC: question\n",
        "conditioned on that input we have an output which is an answer\n",
        "encoder decoder\n",
        "seq to seq: or n to n (same)\n",
        "A B C : rnn steps (LSTM cell feeding) 3 layers deep\n",
        "EOS : encoder (terminal symbol) end of sequence, this is some learned vector\n",
        "embed arbitrary vevtors \n",
        "this then produces a state\n",
        "we use that state in order to decode an output sequence\n",
        "stop the coding we have received the sequence\n",
        "decoding step: softmax that creates a discrete symbol\n",
        "used for translation\n",
        "decoder encoder architecture\n",
        "\n",
        "GRU RNN unit\n",
        "\n",
        "---\n",
        "encoder produces a single output\n",
        "fixed size vector regardless of the input\n",
        "produce output based on the softmax\n",
        "encoder : takes a sequence of symbols which we need to into vectors (embeddings)\n",
        "then passed to the decoding function\n",
        "produces output sequence Y'\n",
        "autoencoder if input=output\n",
        "discrete input sequence and produces a variable length output sequence\n",
        "\n",
        "abstract: learn a mapping into some high dimensional space of a state\n",
        "and that state is completely latent\n",
        "the desired: high dim space that given the decoder the decoder would ...\n",
        "conditioned on the latend embedding the encoder has produced\n",
        "not conditioned on the sequence\n",
        "capacity to learn\n",
        "however in practice this model doesnt always learn that well\n",
        "weights of the encoder learn: weights are shared between the componenet in the encoder\n",
        "and weights are shared inside the decoder\n",
        "weights in the encoder learn: gradient with respect to any component in the network-backprop\n",
        "calculate the difference between the expected prediction (cross entropy loss at each point in the decoder) \n",
        "calculate the gradient with respect to any output cell and then backpropagate\n",
        "EOF or </s>\n",
        "to learn the end sentence character\n",
        "\n",
        "\n",
        "alignment:\n",
        "in machine translation\n",
        "2 sentences in 2 languages\n",
        "some of the words in the input will correspond in some words in the output\n",
        "he corresponds 1-1 in the ..\n",
        "draw a lign same semantic unit\n",
        "\n",
        "no allignment between some words\n",
        "alignment between the tokens and the input and the output\n",
        "manually annotated links\n",
        "\n",
        "a good corpus would have the input sequence and the output seguence ALIGNED\n",
        "(it doesnt have any usage of alignment the encoder and the decoder)\n",
        "seq to seq with attention: there is a correspondance of what you re gonna produce as the output and the input\n",
        "correlation\n",
        "no just through the encoding itself but create a shortcut of information flow\n",
        "take the dot product between the current hidden state (orange arrow) and all of the previous hidden states \n",
        "and then run a softmax \n",
        "to produced a probability distrib\n",
        "highest product - highest correlation between the current state and the previous states\n",
        "i observed\n",
        "state 2: i am\n",
        "state 3: i am a\n",
        "state 4: i am a student\n",
        "(observed)\n",
        "each timestep in the decoder\n",
        "(on;y attention in the decoder, not int he encoder)\n",
        "take the dot product between the hidden state hi dec * hi enc\n",
        "i doesnt change\n",
        "and then calculate the softmax\n",
        "creates a prob distrib over the sequences\n",
        "we take each of the components in the prob distrib and we scale element-wise\n",
        "(each state) by a factor and then sum this elemnt wise : we get a scaled representation of all the previous states\n",
        "concatenate the output and then feed into an mlp and make the prediction\n",
        "thats attention\n",
        "dot product with the encoded version\n",
        "argument \n",
        "create shortucts\n",
        "if we had a good encoder we wouldnt need attention\n",
        "get rid of the encoder entirely\n",
        "decomposable attention\n",
        "idea: no longer do generative model in the decoder side but a classification\n",
        "input set and output seq\n",
        "calculate the dot product between each token\n",
        "and then concatenate and sum them up and feed it into an mlp\n",
        "get rid of the idea of having any reccurence inside the model\n",
        "no weights here, just the dot product between 2 embeddings\n",
        "transformers:\n",
        "what if we add weights to the attention mechanism?\n",
        "still dont have any reccurence (we dont use any sequence of inputs)\n",
        "encoder solely based on attention\n",
        "attention between tokens and weights attached\n",
        "\n",
        "we make embeddings out of inputs\n",
        "input: discrete tokens\n",
        "we map them into embeddings using some embedding matrix\n",
        "then 3 different kind of vectors to calculate attention for the transformer\n",
        "Wq Wk Wv\n",
        "Wq: word mapped into a query embedding\n",
        "then produces a key embedding\n",
        "then maps into a value embedding\n",
        "calculate the dot product but not explicitly in the embeddings itselfs, query vectors: you take the dot product between itseld and the key vector\n",
        "\n",
        "key vector: dot product between...\n",
        "\n",
        "scale the softmax: this is attention\n",
        "stack all these into matrix multiplications\n",
        "matrix of query embeddings\n",
        "different weight matrices: different attention for \"heads\"\n",
        "\n",
        "8 attention heads produced each its output\n",
        "simple matrix matrix multiplication\n",
        "no interdependencies between these calculations\n",
        "each attention head is another embedding with different weights\n",
        "\n",
        "matrix matrix multiplication and element wise operations: do different attention heads in parallel\n",
        "this is why it is good\n",
        "data structures in favored on gpus and tpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1g8ABVcTqcp"
      },
      "source": [
        "## Alignment in machine translation ##\n",
        "\n",
        "However, in 2014 these initial seq2seq models could not compete with “oldschool” statistical machine translation models. Rather, they were seen as proof of concepts that may one day lead to neural models being able to perform high quality translation.\n",
        "\n",
        "![](https://miro.medium.com/max/1522/1*VMsuEe0XNzi2WGxKMnHgew.png)\n",
        "\n",
        "In machine translation it is actually not that common to simply associate an input sequence with an output sequence. Rather, you also *align* input tokens and output tokens. This dates back to at least the 80s with the very first “modern” statistical machine translation models. So, can we draw upon this structure to construct a superior seq2seq model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2yca5w3afee"
      },
      "source": [
        "## seq2seq with attention ##\n",
        "\n",
        "A common pattern in this course has been to take something non-differentiabel with inspiration from previous work or the natural world, what would happen if we did the same thing to the fact that oldschool statistical machine translation models rely on alignment?\n",
        "\n",
        "![](https://user-images.githubusercontent.com/7529838/31751822-86b68320-b4c2-11e7-8f19-165a5ec4c021.png)\n",
        "\n",
        "We have an encoded input sequence:\n",
        "\n",
        "$$ \\textbf{h}^{enc} = \\{ h^{enc}_1, \\ldots, h^{enc}_t \\} $$\n",
        "\n",
        "At each time step in the *decoder*, we take the dot product between our output state and *each* encoded input:\n",
        "\n",
        "$$ \\textbf{a} = \\textrm{softmax}(\\{h^{dec}_i \\cdot h^{enc}_1, \\ldots,  h^{dec}_i \\cdot h^{enc}_t\\}) $$\n",
        "\n",
        "This gives us a probability distribution over $\\textbf{h}^{enc}$ which we can use to scale the contribution of each member of $\\textbf{h}^{enc}$:\n",
        "\n",
        "$$ \\sum_{i = 1}^{t}  a_i h^{enc}_i $$\n",
        "\n",
        "We then use this resulting vector when predicting the next token to output by the decoder.\n",
        "\n",
        "Let us go back to that alignment picture and see how one can interpret it in a new light, that of “soft alignments”.\n",
        "\n",
        "This seemingly simple addition to the seq2seq model by Bahdanau et al. (2015) led to neural machine translation subsuming the field until the present day – although the recent state of the art uses “transformers”, but more on that in a moment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBo3oTvDGrnK"
      },
      "source": [
        "## Decomposable attention ##\n",
        "\n",
        "![](https://miro.medium.com/max/1200/1*ZByTGTbUbqmwyr8M1cAVjg.png)\n",
        "\n",
        "(From [Parikh et al. (2016)](https://www.aclweb.org/anthology/D16-1244.pdf))\n",
        "\n",
        "Attention at this stage was looked at fairly cynically by myself and others as addressing the difficulty of training a model capable of encoding a long sequence into a single vector and then unpacking it – I stick to this view to this day. But a year later the Google Brain group sought to build a *pure* attention-based model, *without any recurrence*.\n",
        "\n",
        "For their task, they had two sequences and was asked to provide a classification decision. What they opted to do was to calculate the attention between each word, then aggregate these interactions, and lastly feed it into a multi-layer perceptron – simple! What was amazing were the results, this model performed as well as models an order of magnitude more weights! This was a shot across the bow against the recurrent neural networks that had dominated natural language processing for the last three years at that point.\n",
        "\n",
        "Can we relate this model to the “bag of vectors” model from Assignment 2 somehow?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BTfoTlnGyVt"
      },
      "source": [
        "## The transformer ##\n",
        "\n",
        "As a next logical step, the same group at Google Brain then moved to introduce weights into the attention mechanism, leading to the model we now commonly know as the transformer.\n",
        "\n",
        "![](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n",
        "\n",
        "A key point to not here, the weights for the feed-forward network are shared.\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_self_attention_score.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/self-attention_softmax.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/self-attention-output.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_attention_heads_z.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n",
        "\n",
        "If we manually inpsect the attention heads, we quickly \n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png)\n",
        "\n",
        "So what about the decoder then… It actually uses the same kind of self-attention units and decodes \n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebZ-VjfCYe64"
      },
      "source": [
        "Most successess have been for Natural Language Processing with models such as GPT-3, but in [“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. (2020) we now have competitive results for image classification as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXN-WGevosXb"
      },
      "source": [
        "## Acknowledgements ##\n",
        "\n",
        "[Jay Alammar’s](https://jalammar.github.io/illustrated-transformer) wonderful “The Illustrated Transformer”."
      ]
    }
  ]
}