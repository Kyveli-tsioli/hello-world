{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "10.01 – Embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyveli-tsioli/hello-world/blob/main/10_01_%E2%80%93_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXbVZCfh5Olh",
        "outputId": "92d8673c-5445-4477-ab03-e549ed778af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Installation cell\n",
        "%%shell\n",
        "if ! command -v julia 2>&1 > /dev/null\n",
        "then\n",
        "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "////\n",
        "# x has some structure of it (in terms of the features)\n",
        "# underlying structure\n",
        "# have x: how to represent this input for our ml model?\n",
        "# as vectors?\n",
        "# a good representation: need to be distinct \n",
        "# we want similar items to have similar representation\n",
        "# items embedded into some space\n",
        "# feature engineering: combination features/adding additional features for external resources: you help the ml system \n",
        "# the most common way make features to have some sort of indicator function:\n",
        "# price of the house? high or low-> binary function -> binary representation on the features\n",
        "# insurance prediction: is from uk? yes/no\n",
        "# look at the space: all these are distinct \n",
        "# input as binary vector \n",
        "# sparse binary representations \n",
        "# one-hot encoding\n",
        "# multi class classification: one-hot encoding \n",
        "# sparse binary features fed direcly into the weights(uncommon)\n",
        "# word2vec: nlp\n",
        "# 2 versions: CBOW/ Skip-gram\n",
        "# 2 inputs: it projects them (calculagtes the element wise sum between each of these vectors)\n",
        "# takes a weight matrix\n",
        "# output: representation of whatever word is missing in the middle\n",
        "# skip-gram: the inverse\n",
        "# take a word, project it, try to reconstruct the context\n",
        "# first instance of unsupervised learning\n",
        "# no labels \n",
        "# statistical semantics \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-03 06:33:59--  https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz\n",
            "Resolving julialang-s3.julialang.org (julialang-s3.julialang.org)... 151.101.2.49, 151.101.66.49, 151.101.130.49, ...\n",
            "Connecting to julialang-s3.julialang.org (julialang-s3.julialang.org)|151.101.2.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88706549 (85M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/julia.tar.gz’\n",
            "\n",
            "/tmp/julia.tar.gz   100%[===================>]  84.60M   187MB/s    in 0.5s    \n",
            "\n",
            "2019-12-03 06:33:59 (187 MB/s) - ‘/tmp/julia.tar.gz’ saved [88706549/88706549]\n",
            "\n",
            "   Cloning default registries into /root/.julia/registries\n",
            "   Cloning registry General from \"https://github.com/JuliaRegistries/General.git\"\n",
            "\u001b[2K\u001b[?25h Resolving package versions...\n",
            " Installed VersionParsing ── v1.1.3\n",
            " Installed Conda ─────────── v1.3.0\n",
            " Installed Parsers ───────── v0.3.10\n",
            " Installed ZMQ ───────────── v1.0.0\n",
            " Installed IJulia ────────── v1.20.2\n",
            " Installed MbedTLS ───────── v0.6.8\n",
            " Installed BinaryProvider ── v0.5.8\n",
            " Installed JSON ──────────── v0.21.0\n",
            " Installed SoftGlobalScope ─ v1.0.10\n",
            " Installed Compat ────────── v2.2.0\n",
            "  Updating `~/.julia/environments/v1.0/Project.toml`\n",
            "  [7073ff75] + IJulia v1.20.2\n",
            "  Updating `~/.julia/environments/v1.0/Manifest.toml`\n",
            "  [b99e7846] + BinaryProvider v0.5.8\n",
            "  [34da2185] + Compat v2.2.0\n",
            "  [8f4d0f93] + Conda v1.3.0\n",
            "  [7073ff75] + IJulia v1.20.2\n",
            "  [682c06a0] + JSON v0.21.0\n",
            "  [739be429] + MbedTLS v0.6.8\n",
            "  [69de0a69] + Parsers v0.3.10\n",
            "  [b85f4697] + SoftGlobalScope v1.0.10\n",
            "  [81def892] + VersionParsing v1.1.3\n",
            "  [c2297ded] + ZMQ v1.0.0\n",
            "  [2a0f44e3] + Base64 \n",
            "  [ade2ca70] + Dates \n",
            "  [8bb1440f] + DelimitedFiles \n",
            "  [8ba89e20] + Distributed \n",
            "  [7b1f6079] + FileWatching \n",
            "  [b77e0a4c] + InteractiveUtils \n",
            "  [76f85450] + LibGit2 \n",
            "  [8f399da3] + Libdl \n",
            "  [37e2e46d] + LinearAlgebra \n",
            "  [56ddb016] + Logging \n",
            "  [d6f4376e] + Markdown \n",
            "  [a63ad114] + Mmap \n",
            "  [44cfe95a] + Pkg \n",
            "  [de0858da] + Printf \n",
            "  [3fa0cd96] + REPL \n",
            "  [9a3f8284] + Random \n",
            "  [ea8e919c] + SHA \n",
            "  [9e88b42a] + Serialization \n",
            "  [1a1011a3] + SharedArrays \n",
            "  [6462fe0b] + Sockets \n",
            "  [2f01184e] + SparseArrays \n",
            "  [10745b16] + Statistics \n",
            "  [8dfed614] + Test \n",
            "  [cf7118a7] + UUIDs \n",
            "  [4ec0a83e] + Unicode \n",
            "  Building Conda ──→ `~/.julia/packages/Conda/kLXeC/deps/build.log`\n",
            "  Building ZMQ ────→ `~/.julia/packages/ZMQ/ABGOx/deps/build.log`\n",
            "  Building MbedTLS → `~/.julia/packages/MbedTLS/X4xar/deps/build.log`\n",
            "  Building IJulia ─→ `~/.julia/packages/IJulia/F1GUo/deps/build.log`\n",
            "Precompiling project...\n",
            "Precompiling IJulia\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uKBM4zJ5OlX",
        "outputId": "fae739e1-ac73-4091-a113-d413a9b34e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "using Pkg\n",
        "\n",
        "pkg\"add Embeddings\"\n",
        "pkg\"add Distances\"\n",
        "\n",
        "pkg\"precompile\"\n",
        "\n",
        "using Embeddings\n",
        "using Distances\n",
        "\n",
        "const w2v = load_embeddings(Word2Vec)\n",
        "\n",
        "nothing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
            "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 3600007168 bytes == 0xa806000 @  0x7f6be8e1eb6b 0x7f6be8e3e379 0x7f6be87315ec 0x7f6be86fb42b 0x7f6bcb2d6ec7 0x7f6bcb2d5e5c 0x7f6be86da05c 0x7f6be86deca7 0x7f6bcb2d59b8 0x7f6bcb2d5710 0x7f6bcb2d57f8 0x7f6be86deca7 0x7f6bcb2d0af9 0x7f6bcb2d0bbb 0x7f6be86deca7 0x7f6bcb2d089a 0x7f6be86de0d6 0x7f6be8847f10 0x7f6be8847c39 0x7f6be88485bc 0x7f6be8848d3f 0x7f6be86f496c 0x7f6be884980d 0x7f6be8713ffc 0x7f6be86ef8e0 0x7f6bd6609a54 0x7f6bd660739d 0x7f6be86de0d6 0x7f6be86ec846 0x7f6be86ecea2 0x7f6bd65fd3e9\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900dqb9wKoXJ"
      },
      "source": [
        "# Embeddings #\n",
        "\n",
        "Up until this point we have dealt with continuous data (images, measurements, etc.), but a healthy amount of data one can be interested in is discrete. Think of graph structures, language, etc. These pose a challenge for neural models, why?\n",
        "\n",
        "The vocabulary of a model for language is frequently in the millions and a big knowledge graph like Wikidata contains [66,689,671](https://www.wikidata.org/wiki/Wikidata:Statistics) entities.\n",
        "\n",
        "What does a good representation look like in an abstract sense?:\n",
        "\n",
        "1. Representations are **distinct**\n",
        "\n",
        "2. Similar items have **similar** representations\n",
        "\n",
        "If both of these statements hold, it is likely that a representation can be useful for a downstream machine learning system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsxD0VxOQjPN"
      },
      "source": [
        "## An indicator function ##\n",
        "\n",
        "For more “traditional” machine learning systems, we are likely to use an indicator function such as:\n",
        "\n",
        "$$f_{id}(w) \\mapsto \\mathbb{N^{*}}$$\n",
        "\n",
        "We can then create a sparse binary representation using this function:\n",
        "\n",
        "$$g(w, i) = {\\left\\lbrace\n",
        "                            \\begin{array}{ll}\n",
        "                                1 & \\textrm{if }~i = f_{id}(w) \\\\\n",
        "                                0 & \\textrm{otherwise} \\\\\n",
        "                            \\end{array}\\right.}$$\n",
        "\n",
        "$$f_{sb}(w) = [g(w, 1), \\ldots]$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5U4A4ObR36G"
      },
      "source": [
        "As a concrete “toy” example, we can imagine:\n",
        "\n",
        "$$\\mathbb{V} = \\{\\textrm{cat}, \\textrm{dog}, \\textrm{human}\\}$$\n",
        "\n",
        "$$ f_{id}(\\textrm{cat}) = 1, \\ldots, f_{id}(\\textrm{human}) = 3$$\n",
        "\n",
        "$$f_{sb}(\\textrm{cat}) = [1, 0, 0]$$\n",
        "\n",
        "$$f_{sb}(\\textrm{dog}) = [0, 1, 0]$$\n",
        "\n",
        "$$f_{sb}(\\textrm{human}) = [0, 0, 1]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKkQojUW182j"
      },
      "source": [
        "## A case study: word2vec ##\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*GkL9XH1XXSUU0RfAHKJGCg.png)\n",
        "\n",
        "[Image source](https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf)\n",
        "\n",
        "“If A and B have almost identical environments we say that they are synonyms” – Zellig Harris (1954)\n",
        "\n",
        "“You shall know a word by the company it keeps” – John Rupert Firth (1957)\n",
        "\n",
        "For some intuition, consider: “I had some _____ for breakfast today”, you instinctively know that “airplanes” is likely to be wrong, but “toast” is likely to be right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjVP6OxVF8BT"
      },
      "source": [
        "### Formalised ###\n",
        "\n",
        "$$ W^w \\in \\mathbb{R}^{|\\mathbb{V}| \\times d} $$\n",
        "\n",
        "$$ w \\in \\mathbb{V} $$\n",
        "\n",
        "$$ W^c \\in \\mathbb{R}^{|\\mathbb{V}| \\times d} $$\n",
        "\n",
        "$$ c \\in \\mathbb{V} $$\n",
        "\n",
        "$$ σ(x)  = \\frac{1}{1 + e^{-x}} $$\n",
        "\n",
        "$$ \\mathcal{D} = \\{(w_1, c_1), \\ldots, (w_n, c_n)\\} $$\n",
        "\n",
        "$$ \\mathcal{D}^{\\prime} = \\{(w_1, c_1), \\ldots, (w_m, c_m)\\} $$\n",
        "\n",
        "$$ p((c, w) \\in \\mathcal{D}|c, w, W^{w}, W^{c}) = \\sigma(W^{c}_{f_{id}(c), :} \\cdot W^{w}_{f_{id}(w), :}) $$\n",
        "\n",
        "$$ arg\\max_{w,b}\n",
        "    \\sum_{(w, c) \\in D} \\textrm{log}(W^{c}_{f_{id}(c), :} \\cdot W^{w}_{f_{id}(w), :})\n",
        "    + \\sum_{(w, c) \\in D^{\\prime}} \\textrm{log}(-W^{c}_{f_{id}(c), :} \\cdot W^{c}_{f_{id}(w), :})\n",
        "$$\n",
        "\n",
        "Why is $\\mathcal{D}^{\\prime}$ necessary? What happens if we remove it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G_Gp558x3yq"
      },
      "source": [
        "const token2index = Dict(token => i for (i, token) in enumerate(w2v.vocab))\n",
        "embedding(token)  = w2v.embeddings[:, token2index[token]]\n",
        "\n",
        "nothing\n",
        "\n",
        "\n",
        "# creating representations from data\n",
        "# non examinable\n",
        "# huge weight vector W\n",
        "# embedding \n",
        "# through any text collection we have D\n",
        "# collect these tuples: w together with word 1\n",
        "# degine contextual window (distance of five) D'\n",
        "# no labels here\n",
        "# prob that this word has been observed from data \n",
        "# D' (d prime): a collection of words and context words instead of collecting the words already seen\n",
        "# use sample uniformly (use frequency of words, distribution)\n",
        "# w(t-2) ... similar to w(t): want\n",
        "# w(t) not observed we want to be disimilar to the context of w(t-2), w(t-1) w(t+1) w(t+2)\n",
        "# Wc signifies a column inseide the weight matrix\n",
        "# apply the indicator function and you peak out a row outside of the weight matrix \n",
        "# Wc d dimensional vectors which are rows inside the matrices\n",
        "# without negative examples our model could simply predict each word to appear with high probability\n",
        "# so we need to have negative examples\n",
        "# we can now learn representations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6c35WRAbA7d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvsW0jq8NPip"
      },
      "source": [
        "### Cosine distance ###\n",
        "\n",
        "$$ cos(u, v) = 1 - \\frac{u \\cdot v}{||u|| ||v||} $$\n",
        "\n",
        "$$ cos(u, v) \\mapsto [0, 2] $$\n",
        "\n",
        "$$ cos(u, v) = 0 \\implies \\textrm{identical} $$\n",
        "\n",
        "$$ cos(u, v) = 2 \\implies \\textrm{opposites} $$\n",
        "\n",
        "$$ cos(u, v) = 1 \\implies \\textrm{orthogonal} $$\n",
        "\n",
        "Why are we not using something more “standard” like the Euclidian distance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPQ6Rwzo2Jrf",
        "outputId": "a49bd92b-6f88-47c7-90d1-e5e02aa6b857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "distance(tokena, tokenb) = cosine_dist(embedding(tokena), embedding(tokenb))\n",
        "\n",
        "@show distance(\"dog\", \"cat\")\n",
        "@show distance(\"cat\", \"dog\") # Symmetric, of course.\n",
        "@show distance(\"cat\", \"human\")\n",
        "@show distance(\"dog\", \"human\")\n",
        "@show distance(\"ape\", \"human\");\n",
        "\n",
        "\n",
        "# it gives a value between 0 and 2\n",
        "# if you have value of 0 means that you have two identical vectors (same direction)\n",
        "# if a value of 2 (similar but different directions)\n",
        "# d directionality\n",
        "# vectors of large magnitude \n",
        "# its a worst to mask differences, only changes directionality of the vectors\n",
        "# thats why euclidean\n",
        "# 0.32 \n",
        "# pca or tsni\n",
        "# takes the 1-d vector (1000 dimensional)\n",
        "# and project them into 2 dimensions\n",
        "# similar words have similar representations\n",
        "# n=32 finds the first 32 nearest neighbors of a word (like synonyms)\n",
        "# takes into account some collocations (\"cats and dogs\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance(\"dog\", \"cat\") = 0.2390545f0\n",
            "distance(\"cat\", \"dog\") = 0.2390545f0\n",
            "distance(\"cat\", \"human\") = 0.75563335f0\n",
            "distance(\"dog\", \"human\") = 0.8109204f0\n",
            "distance(\"ape\", \"human\") = 0.6500703f0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8NY54-r_5qY"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/1600/1*vvtIsW1AblmgLkq1peKfOg.png)\n",
        "\n",
        "[Image source](https://towardsdatascience.com/mapping-word-embeddings-with-word2vec-99a799dc9695)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwJwp4ua2oC3"
      },
      "source": [
        "## Neighbourhoods ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve2Q_scR1NFF"
      },
      "source": [
        "function neighbourhood(emb; n=32)\n",
        "    nneighbours = Int[]\n",
        "    maxdist     = typemax(Float32)\n",
        "    for i in eachindex(w2v.vocab)\n",
        "        d = cosine_dist(emb, w2v.embeddings[:, i])\n",
        "        if length(nneighbours) > n && d >= maxdist\n",
        "            continue\n",
        "        end\n",
        "        push!(nneighbours, i)\n",
        "        sort!(nneighbours,\n",
        "            by=j -> cosine_dist(emb, w2v.embeddings[:, j]))\n",
        "        maxdist = cosine_dist(emb, w2v.embeddings[:, nneighbours[end]])\n",
        "        length(nneighbours) <= n || pop!(nneighbours)\n",
        "    end\n",
        "    [(w2v.vocab[i], cosine_dist(emb, w2v.embeddings[:, i]))\n",
        "        for i in nneighbours]\n",
        "end\n",
        "neighbourhood(token::String; n=32) = neighbourhood(embedding(token), n=n)\n",
        "\n",
        "nothing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn45Jdur2nqv",
        "outputId": "08fdf8db-eb43-42e6-eb5b-1be691e55199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "neighbourhood(\"dog\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32-element Array{Tuple{String,Float32},1}:\n",
              " (\"dog\", 0.0)               \n",
              " (\"dogs\", 0.13195097)       \n",
              " (\"puppy\", 0.18935716)      \n",
              " (\"pooch\", 0.23726177)      \n",
              " (\"cat\", 0.2390545)         \n",
              " (\"Rottweiler\", 0.25623828) \n",
              " (\"beagle\", 0.25813752)     \n",
              " (\"pup\", 0.25930876)        \n",
              " (\"Pomeranian\", 0.27613777) \n",
              " (\"canines\", 0.27787548)    \n",
              " (\"chihuahua\", 0.2826082)   \n",
              " (\"pet\", 0.28352147)        \n",
              " (\"schnauzer\", 0.28719038)  \n",
              " ⋮                          \n",
              " (\"bullmastiff\", 0.31103432)\n",
              " (\"sheltie\", 0.322847)      \n",
              " (\"Staffie\", 0.32408535)    \n",
              " (\"Shitzu\", 0.32796305)     \n",
              " (\"collie\", 0.32855868)     \n",
              " (\"Rottweiller\", 0.33353508)\n",
              " (\"kitten\", 0.3340115)      \n",
              " (\"cats\", 0.3346824)        \n",
              " (\"puppies\", 0.33629328)    \n",
              " (\"kennel\", 0.33641297)     \n",
              " (\"pug\", 0.33881402)        \n",
              " (\"Rotweiller\", 0.33903676) "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IOZZkIS93fY",
        "outputId": "57bc989a-bb5b-4955-f9a4-de1304133251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "neighbourhood(\"england\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32-element Array{Tuple{String,Float32},1}:\n",
              " (\"england\", 0.0)         \n",
              " (\"liverpool\", 0.28275466)\n",
              " (\"chelsea\", 0.29395294)  \n",
              " (\"fulham\", 0.29792225)   \n",
              " (\"tottenham\", 0.3092957) \n",
              " (\"rooney\", 0.3136869)    \n",
              " (\"torres\", 0.32362872)   \n",
              " (\"ronaldo\", 0.3242663)   \n",
              " (\"spain\", 0.32593936)    \n",
              " (\"gerrard\", 0.32909578)  \n",
              " (\"anelka\", 0.3319348)    \n",
              " (\"fergie\", 0.3326596)    \n",
              " (\"europe\", 0.335052)     \n",
              " ⋮                        \n",
              " (\"newcastle\", 0.3432166) \n",
              " (\"leeds\", 0.34379578)    \n",
              " (\"barca\", 0.34468734)    \n",
              " (\"madrid\", 0.3449734)    \n",
              " (\"everton\", 0.34627277)  \n",
              " (\"juve\", 0.3468153)      \n",
              " (\"wenger\", 0.34718215)   \n",
              " (\"benitez\", 0.3498447)   \n",
              " (\"beckham\", 0.3502348)   \n",
              " (\"henry\", 0.35096765)    \n",
              " (\"italy\", 0.3517356)     \n",
              " (\"pompey\", 0.35407293)   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NGUodnstd-a",
        "outputId": "990e7da3-67e3-40b2-8621-a1f2cf3f120a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "neighbourhood(\"neuron\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32-element Array{Tuple{String,Float32},1}:\n",
              " (\"neuron\", 0.0)                  \n",
              " (\"neurons\", 0.14138311)          \n",
              " (\"neuronal\", 0.23898268)         \n",
              " (\"neurones\", 0.25595534)         \n",
              " (\"synapse\", 0.26050967)          \n",
              " (\"synapses\", 0.2702875)          \n",
              " (\"neural\", 0.28256142)           \n",
              " (\"neurone\", 0.29190993)          \n",
              " (\"synaptic\", 0.3098709)          \n",
              " (\"cortex\", 0.32931906)           \n",
              " (\"Neurons\", 0.34913397)          \n",
              " (\"axons\", 0.35264045)            \n",
              " (\"axon\", 0.36290532)             \n",
              " ⋮                                \n",
              " (\"brain\", 0.3841828)             \n",
              " (\"forebrain\", 0.38504547)        \n",
              " (\"BDNF\", 0.38742512)             \n",
              " (\"astrocyte\", 0.38816023)        \n",
              " (\"hippocampal\", 0.39035177)      \n",
              " (\"hippocampus\", 0.39344954)      \n",
              " (\"presynaptic\", 0.39414895)      \n",
              " (\"astrocytes\", 0.39785504)       \n",
              " (\"neurogenesis\", 0.39950663)     \n",
              " (\"molecule\", 0.39971977)         \n",
              " (\"neurotransmitters\", 0.39981014)\n",
              " (\"MEF2\", 0.40184808)             "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEzm66Wtziyt"
      },
      "source": [
        "### Word representation algebra ###\n",
        "\n",
        "$$ f_{n}(\\textrm{king}) - f_{n}(\\textrm{man}) + f_{n}(\\textrm{woman}) \\approx f_{n}(\\textrm{queen}) $$\n",
        "\n",
        "$$ f_{n}(\\textrm{Paris}) - f_{n}(\\textrm{France}) + f_{n}(\\textrm{Italy}) \\approx f_{n}(\\textrm{Rome})$$\n",
        "\n",
        "This is notoriously difficult to replicate though…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmv_HywN9-JX",
        "outputId": "7bec7e1c-c964-4f59-cefd-f6cc14fa409d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "isto(ais, tob, likecto) = neighbourhood(embedding(ais) .- embedding(tob) .+ embedding(likecto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "isto (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Uyr7cm-yH2",
        "outputId": "f884116f-b589-464f-b00e-1ac4142ab313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "isto(\"king\", \"man\", \"woman\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32-element Array{Tuple{String,Float32},1}:\n",
              " (\"king\", 0.2007401)         \n",
              " (\"queen\", 0.28818095)       \n",
              " (\"monarch\", 0.3810326)      \n",
              " (\"princess\", 0.4097572)     \n",
              " (\"prince\", 0.462268)        \n",
              " (\"kings\", 0.4763158)        \n",
              " (\"queens\", 0.4818864)       \n",
              " (\"sultan\", 0.4901408)       \n",
              " (\"monarchy\", 0.49125886)    \n",
              " (\"throne\", 0.49941933)      \n",
              " (\"royal\", 0.5061796)        \n",
              " (\"ruler\", 0.50907254)       \n",
              " (\"empress\", 0.5112186)      \n",
              " ⋮                           \n",
              " (\"Mswati\", 0.5552143)       \n",
              " (\"maharaja\", 0.55632824)    \n",
              " (\"princesses\", 0.5576925)   \n",
              " (\"Princess\", 0.5624594)     \n",
              " (\"duchess\", 0.5627598)      \n",
              " (\"Queen\", 0.5653622)        \n",
              " (\"monarchs\", 0.5656772)     \n",
              " (\"handmaid\", 0.5685401)     \n",
              " (\"Gosakuramachi\", 0.5699681)\n",
              " (\"kumari\", 0.57787603)      \n",
              " (\"maharani\", 0.57856727)    \n",
              " (\"commoner\", 0.57872516)    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PNfVP1M_XHh",
        "outputId": "ff25be21-f2e0-4b1a-ea65-1334560e9ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "isto(\"Paris\", \"France\", \"Italy\")\n",
        "# matrix decomposition\n",
        "# d hyperparameter dimensionality\n",
        "# transfer learning\n",
        "# weight matrices: each row is a d dimensional vector that corresponds to a word\n",
        "# corresponds to a vocabulary\n",
        "# it does not represent a single word, but a whole vocabulary\n",
        "# rows: used when we calculate the probability of this word in the context (in the data)\n",
        "# v is the number of the words?\n",
        "# weights representing the words\n",
        "# "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32-element Array{Tuple{String,Float32},1}:\n",
              " (\"Milan\", 0.27778602)  \n",
              " (\"Rome\", 0.2971689)    \n",
              " (\"Italy\", 0.3267352)   \n",
              " (\"Paris\", 0.3448093)   \n",
              " (\"Italian\", 0.4088726) \n",
              " (\"Tuscany\", 0.43671834)\n",
              " (\"Bologna\", 0.43916416)\n",
              " (\"Sicily\", 0.44036168) \n",
              " (\"Genoa\", 0.46910983)  \n",
              " (\"Milanese\", 0.4694308)\n",
              " (\"Rimini\", 0.48378998) \n",
              " (\"Venice\", 0.4858547)  \n",
              " (\"ANSA\", 0.4889533)    \n",
              " ⋮                      \n",
              " (\"Pisa\", 0.499672)     \n",
              " (\"Liguria\", 0.5008726) \n",
              " (\"Sassari\", 0.50194716)\n",
              " (\"Brianza\", 0.50275654)\n",
              " (\"Sempione\", 0.5056912)\n",
              " (\"Pietro\", 0.5061012)  \n",
              " (\"Pomezia\", 0.50707614)\n",
              " (\"Giorgio\", 0.5097326) \n",
              " (\"Vicenza\", 0.5102454) \n",
              " (\"Gianni\", 0.51183736) \n",
              " (\"Pistoia\", 0.51197916)\n",
              " (\"Jacopo\", 0.51241815) "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXGae9WbBC-U"
      },
      "source": [
        "![](https://samyzaf.com/ML/nlp/word2vec2.png)\n",
        "\n",
        "[Image source](https://samyzaf.com/ML/nlp/nlp.html)\n",
        "\n",
        "Why and how would these relationships be preserved in the embeddings? Think, what do they capture?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg6RxeA8BMhD"
      },
      "source": [
        "**Beware** when playing around with these models though, they are a reflection of the underlying data and very well mirrors the worst of us as a species in our racism, sexism, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffnmf-Lo0Szd"
      },
      "source": [
        "## Some closing comments and questions… ##\n",
        "\n",
        "The same line of thinking case be used for graph embeddings, etc. I also find it helpful to think about what an MLP or CNN achieves in a similar way.\n",
        "\n",
        "1. How do we handle sequences?\n",
        "\n",
        "2. Our representations are context independent, is this a problem?\n",
        "\n",
        "3. Can we imagine loss functions akin to word2vec for other areas?\n",
        "\n",
        "4. From your Linear Algebra, does $W^w$ and $W^c$ remind you of something? Does this hint at another way to learn embeddings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fix-wADHz8wP"
      },
      "source": [
        "## Extracurricular reading ##\n",
        "\n",
        "1. [“Distributed Representations of Words and Phrases and their Compositionality”](https://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases) by Mikolov et al. (2013)\n",
        "\n",
        "2. [“word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method”](https://arxiv.org/abs/1402.3722) by Goldberg and Levy (2014). I *highly* recommend to **read this in parallel with Mikolov et al. (2013)**."
      ]
    }
  ]
}